{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "# open twitter sentiment data\n",
    "# 1 = positive\n",
    "# 0 = Negative\n",
    "twitter_data_df = pd.read_csv(\"../data/raw/Twitter_Sentiment_Analysis/Sentiment Analysis Dataset.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = twitter_data_df.SentimentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_all = twitter_data_df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578612, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t, X_val, Y_t, Y_val = train_test_split(X_all,Y_all, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_t,Y_t, test_size=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126289,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once this is done, we need to convert the sentences as a one-hot tensor of shape [sentence_length x word_length x alphabet_size].\n",
    "- https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_ALPHABET = 'abcdefghijklmnopqrstuvwxyz0123456789-,;' \\\n",
    "               '.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "MAX_WORD_LENGTH = 16 # number of characters in a word\n",
    "ALPHABET_SIZE = len(EMB_ALPHABET)\n",
    "# we associate every character in our alphabet to a number: \n",
    "# e.g. b => 1 d => 3 etc.\n",
    "ALPHABET_DICT = {ch: ix for ix, ch in enumerate(EMB_ALPHABET)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_one_hot( sentence, emb_alphabet, max_word_length, alphabet_size, alphabet_dict):\n",
    "    \"\"\"Convert a sentence to a one hot character encoding tensor for that sentence using alphanumeric characters\"\"\"\n",
    "    # https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/\n",
    "    # Convert Sentences to np.array of Shape \n",
    "    # ('sent_length', 'word_length', 'emb_size')\n",
    "\n",
    "    sent = []\n",
    "\n",
    "    # We need to keep track of the maximum length of the sentence in a minibatch\n",
    "    # so that we can pad them with zeros, this is why we return the length of every\n",
    "    # sentences after they are converted to one-hot tensors\n",
    "    SENT_LENGTH = 0\n",
    "\n",
    "    # Here, we remove any non-printable characters in a sentence (mostly\n",
    "    # non-ASCII characters)\n",
    "    printable = emb_alphabet\n",
    "    encoded_sentence = filter(lambda x: x in printable, sentence)\n",
    "\n",
    "    # word_tokenize() splits a sentence into an array where each element is\n",
    "    # a word in the sentence, for example, \n",
    "    # \"My name is Charles\" => [\"My\", \"name\", \"is\", Charles\"]\n",
    "    blob = TextBlob(sentence)\n",
    "    individual_words_from_sentence = blob.tokenize()\n",
    "    for word in individual_words_from_sentence :\n",
    "\n",
    "        # Encode one word as a matrix of shape [max_word_length x ALPHABET_SIZE]\n",
    "        word_encoding = np.zeros(shape=(max_word_length, alphabet_size))\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "\n",
    "            # If the character is not in the alphabet, ignore it    \n",
    "            try:\n",
    "                char_encoding = alphabet_dict[char]\n",
    "                one_hot = np.zeros(ALPHABET_SIZE)\n",
    "                one_hot[char_encoding] = 1\n",
    "                word_encoding[i] = one_hot\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        sent.append(np.array(word_encoding))\n",
    "        SENT_LENGTH += 1\n",
    "\n",
    "    return np.array(sent), SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array,length = encode_one_hot(X_all[1], EMB_ALPHABET , MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT)\n",
    "# array is number of words in sentence X Max word length  X one hot encoding for alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad tensors to all have the same size \n",
    "- [batch_size x maximum_sentence_length x maximum_word_length x alphabet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   is so sad for my APL friend............'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[0][2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_minibatch( sentences, sentiment_y,  max_word_length, alphabet_size):\n",
    "    \"\"\"Create a minibath of one-hot encoded sentences \n",
    "    array is Batch Size X max number of words in sentence X Max word length  X one hot encoding for alphabet,\n",
    "    \n",
    "    and one hot encoded y [0,1] or [1,0] whre 1=positive\"\"\"\n",
    "    # Create a minibatch of sentences and convert sentiment\n",
    "    # to a one-hot vector, also takes care of padding\n",
    "\n",
    "    max_word_length = max_word_length\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    max_length = 0\n",
    "\n",
    "    for sentence,sent in zip(sentences, sentiment_y):\n",
    "        # Append the one-hot encoding of the sentiment to the minibatch of Y\n",
    "        # 0: Negative 1: Positive\n",
    "        minibatch_y.append(np.array([0, 1]) if sent == 0 else np.array([1, 0]))\n",
    "\n",
    "        # One-hot encoding of the sentence\n",
    "        one_hot, length = encode_one_hot(sentence,EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )\n",
    "\n",
    "        # Calculate maximum_sentence_length\n",
    "        if length >= max_length:\n",
    "            max_length = length\n",
    "\n",
    "        # Append encoded sentence to the minibatch of X\n",
    "        minibatch_x.append(one_hot)\n",
    "\n",
    "\n",
    "    # data is a np.array of shape ('b', 's', 'w', 'e') we want to\n",
    "    # pad it with np.zeros of shape ('e',) to get \n",
    "    # ('b', 'SENTENCE_MAX_LENGTH', 'WORD_MAX_LENGTH', 'e')\n",
    "    def numpy_fillna(data):\n",
    "        \"\"\" This is a very useful function that fill the holes in our tensor \"\"\"\n",
    "\n",
    "        # Get lengths of each row of data\n",
    "        lens = np.array([len(i) for i in data])\n",
    "\n",
    "        # Mask of valid places in each row\n",
    "        mask = np.arange(lens.max()) < lens[:, None]\n",
    "        #print(mask)\n",
    "\n",
    "        # Setup output array and put elements from data into masked positions\n",
    "        out = np.zeros(shape=(mask.shape + (max_word_length, alphabet_size)),\n",
    "                       dtype='float32')\n",
    "        #print(out)\n",
    "\n",
    "        out[mask] = np.concatenate(data)\n",
    "        #print(out,'final')\n",
    "        return out\n",
    "\n",
    "    # Padding...\n",
    "    minibatch_x = numpy_fillna(minibatch_x)\n",
    "\n",
    "    return minibatch_x, np.array(minibatch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_x, minibatch_y = make_minibatch(X_test[:15],Y_test[:15], MAX_WORD_LENGTH, ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 31, 16, 69)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a generator to feed to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatch(batch_size, X_data, y_data):\n",
    "        \"\"\" Returns Next Batch \"\"\"\n",
    "        \n",
    "        n_samples = len(X_data)\n",
    "        \n",
    "        # Number of batches / number of iterations per epoch\n",
    "        n_batch = int(n_samples // batch_size)\n",
    "        \n",
    "        # Creates a minibatch, loads it to RAM and feed it to the network\n",
    "        # until the buffer is empty\n",
    "        for i in range(n_batch):\n",
    "\n",
    "            inputs, targets = make_minibatch(X_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             y_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             MAX_WORD_LENGTH, ALPHABET_SIZE)\n",
    "            yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "    \"\"\" Straight-forward convvolutional layer \"\"\"\n",
    "    # w is the kernel, b the bias, no strides and VALID padding\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "        b = tf.get_variable('b', [output_dim])\n",
    "\n",
    "    return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdnn(input_, kernels, kernel_features, ALPHABET_SIZE, scope='TDNN'):\n",
    "    ''' Time Delay Neural Network\n",
    "    :input:           input float tensor of shape \n",
    "                      [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "    :kernels:         array of kernel sizes\n",
    "    :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "    '''\n",
    "    assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "\n",
    "    # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'embed_size') we\n",
    "    # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
    "    # use conv2D\n",
    "    # It might not seem obvious why we need to use this small hack at first sight, the reason\n",
    "    # is that sentence_length will change across the different minibatches, but if we kept it\n",
    "    # as is sentence_length would act as the number of channels in the convnet which NEEDS to\n",
    "    # stay the same\n",
    "    input_ = tf.reshape(input_, [-1, MAX_WORD_LENGTH, ALPHABET_SIZE])\n",
    "    input_ = tf.expand_dims(input_, 1)\n",
    "\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope):\n",
    "        for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "            reduced_length = MAX_WORD_LENGTH - kernel_size + 1\n",
    "\n",
    "            # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
    "            conv = conv2d(input_, kernel_feature_size, 1,\n",
    "                          kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "\n",
    "            # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
    "            pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "            layers.append(tf.squeeze(pool, [1, 2]))\n",
    "\n",
    "        if len(kernels) > 1:\n",
    "            output = tf.concat(layers, 1)\n",
    "        else:\n",
    "            output = layers[0]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This goes in ops.py\n",
    "\n",
    "def linear(input_, output_size, scope=None):\n",
    "    \"\"\"\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "    \n",
    "    Args:\n",
    "        args: a tensor or a list of 2D, batch x n, Tensors.\n",
    "        output_size: int, second dimension of W[i].\n",
    "        scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "        \n",
    "    Returns:\n",
    "        A 2D Tensor with shape [batch x output_size] equal to\n",
    "        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size],\n",
    "                                 dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway network to help train deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(object):\n",
    "    \"\"\" Character-Level LSTM Implementation \"\"\"\n",
    "\n",
    "    def __init__(self, hparams=None):\n",
    "        # Get the hyperparameters\n",
    "        self.hparams = self.get_hparams()\n",
    "        \n",
    "        # maximum length of each words\n",
    "        max_word_length = self.hparams['max_word_length']\n",
    "        \n",
    "        # X is of shape ('b', 'sentence_length', 'max_word_length', 'alphabet_size']\n",
    "        # Placeholder for the one-hot encoded sentences\n",
    "        X = tf.placeholder('float32', \n",
    "                                shape=[None, None, max_word_length, ALPHABET_SIZE],\n",
    "                                name='X')\n",
    "        self.X = X\n",
    "        \n",
    "        # Placeholder for the one-hot encoded sentiment\n",
    "        Y = tf.placeholder('float32', shape=[None, 2], name='Y')\n",
    "        self.Y = Y\n",
    "        \n",
    "    def get_hparams(self):\n",
    "        ''' Get Hyperparameters '''\n",
    "\n",
    "        return {\n",
    "            'BATCH_SIZE':       64,\n",
    "            'EPOCHS':           500,\n",
    "            'max_word_length':  16,\n",
    "            'learning_rate':    0.0001,\n",
    "            'patience':         10000,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lstm.X # tf placeholder \n",
    "Y = lstm.Y # tf placeholder for class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input_, out_dim, scope=None):\n",
    "    \"\"\" SoftMax Output \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope or 'softmax'):\n",
    "        W = tf.get_variable('W', [input_.get_shape()[1], out_dim])\n",
    "        b = tf.get_variable('b', [out_dim])\n",
    "\n",
    "    return tf.nn.softmax(tf.matmul(input_, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126289,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = None # class variable\n",
    "train_samples=1136600\n",
    "valid_samples = 126289\n",
    "def build(\n",
    "              training=True,\n",
    "              testing_batch_size=1000,\n",
    "              kernels=[1, 2, 3, 4, 5, 6, 7],\n",
    "              kernel_features=[25, 50, 75, 100, 125, 150, 175],\n",
    "              rnn_size=650,\n",
    "              dropout=0.0,\n",
    "              size=700,\n",
    "              train_samples=train_samples,\n",
    "              valid_samples=valid_samples,\n",
    "              hparams= lstm.get_hparams()):\n",
    "        \"\"\"\n",
    "        Build the computational graph\n",
    "        \n",
    "        :param training: \n",
    "            Boolean whether we are training (True) or testing (False)\n",
    "        \n",
    "        :param testing_batch_size: \n",
    "            Batch size to use during testing \n",
    "            \n",
    "        :param kernels:\n",
    "            Kernel width for each convolutional layer\n",
    "         \n",
    "        :param kernel_features: \n",
    "            Number of kernels for each convolutional layer\n",
    "            \n",
    "        :param rnn_size: \n",
    "            Size of the LSTM output\n",
    "        \n",
    "        :param dropout: \n",
    "            Retain probability when using dropout\n",
    "        \n",
    "        :param size: \n",
    "            Size of the Highway embeding\n",
    "        \n",
    "        :param train_samples: \n",
    "            Number of training samples\n",
    "        \n",
    "        :param valid_samples: \n",
    "            Number of validation samples\n",
    "        :hyperparameters\n",
    "            dict of parameters\n",
    "        \"\"\"\n",
    "\n",
    "        max_word_length = hparams['max_word_length']\n",
    "        size = size\n",
    "        max_word_length = hparams['max_word_length']\n",
    "        train_samples = train_samples\n",
    "        valid_samples = valid_samples\n",
    "\n",
    "        \n",
    "        # If we are training use the BATCH_SIZE from the hyperparameters\n",
    "        # else use the testing batch size\n",
    "        if training == True:\n",
    "            BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "\n",
    "        else:\n",
    "            BATCH_SIZE = testing_batch_size\n",
    "\n",
    "        \n",
    "        # Pass the sentences through the CharCNN network\n",
    "        cnn = tdnn(X, kernels, kernel_features, ALPHABET_SIZE) # X is a TF placeholder from the LSTM network\n",
    "\n",
    "        # tdnn() returns a tensor of shape [batch_size * sentence_length x kernel_features]\n",
    "        # highway() returns a tensor of shape [batch_size * sentence_length x size] to use\n",
    "        # tensorflow dynamic_rnn module we need to reshape it to \n",
    "        # [batch_size x sentence_length x size]\n",
    "        cnn = highway(cnn, size)\n",
    "\n",
    "        cnn = tf.reshape(cnn, [BATCH_SIZE, -1, size])\n",
    "\n",
    "        \n",
    "        # Build the LSTM\n",
    "        with tf.variable_scope('LSTM'):\n",
    "        \n",
    "            # The following is pretty straight-forward, create a cell and add dropout if\n",
    "            # necessary. Note that I did not use dropout to get my results, but using it\n",
    "            # will probably help\n",
    "            def create_rnn_cell():\n",
    "                cell = rnn.BasicLSTMCell(rnn_size, state_is_tuple=True,\n",
    "                                         forget_bias=0.0, reuse=False)\n",
    "\n",
    "                if dropout > 0.0:\n",
    "                    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1. - dropout)\n",
    "\n",
    "                return cell\n",
    "\n",
    "            cell = create_rnn_cell()\n",
    "            \n",
    "            # Initial state of the LSTM cell\n",
    "            initial_rnn_state = cell.zero_state(BATCH_SIZE, dtype='float32')\n",
    "            \n",
    "            # This function returns the outputs at every steps of \n",
    "            # the LSTM (i.e. one output for every word)\n",
    "            outputs, final_rnn_state = tf.nn.dynamic_rnn(cell, cnn,\n",
    "                                                         initial_state=initial_rnn_state,\n",
    "                                                         dtype=tf.float32)\n",
    "\n",
    "\n",
    "            # In this implementation, we only care about the last outputs of the RNN\n",
    "            # i.e. the output at the end of the sentence\n",
    "            outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "            last = outputs[-1]\n",
    "        global prediction\n",
    "        prediction = softmax(last, 2)\n",
    "        \n",
    "\n",
    "\n",
    "size = None\n",
    "hparams = None\n",
    "max_word_length = None\n",
    "\n",
    "\n",
    "build(hparams=lstm.get_hparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(64, 2) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PATH = '/NOBACKUP/ashbylepoc/PycharmProjects/CharLSTM/'\n",
    "# TRAIN_SET = PATH + 'datasets/train_set.csv'\n",
    "# TEST_SET = PATH + 'datasets/test_set.csv'\n",
    "# VALID_SET = PATH + 'datasets/valid_set.csv'\n",
    "\n",
    "SAVE_PATH = 'tensorflow/lstm'\n",
    "LOGGING_PATH = 'tensorflow/log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hparams, X_train, Y_train, X_test, Y_test, load_model = False):\n",
    "    \n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "    EPOCHS = hparams['EPOCHS']\n",
    "    max_word_length = hparams['max_word_length']\n",
    "    learning_rate = hparams['learning_rate']\n",
    "\n",
    "    # the probability for each sentiment (pos, neg)\n",
    "    pred = prediction\n",
    "\n",
    "    # Binary cross-entropy loss\n",
    "    cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "    # The number of \"predictions\" we got right, we assign a sentence with \n",
    "    # a positive connotation when the probability to be positive is greater then\n",
    "    # the probability of being negative and vice-versa.\n",
    "    predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    # Accuracy: # predictions right / total number of predictions\n",
    "    acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "    # We use the Adam Optimizer\n",
    "    #tf.get_variable_scope().reuse_variables()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='AdAm212').minimize(cost)\n",
    "    \n",
    "#     try: # See if this varaible is already defined\n",
    "        \n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,name ='ADAM_NEW').minimize(cost)\n",
    "#         #tf.get_variable_scope(reuse=None).reuse_variables()\n",
    "#     except Exception as e:\n",
    "#         #optimizer = tf.get_variable('TDNN/kernel_1/w/Adam_4/')\n",
    "#         print(e)\n",
    "#         pass\n",
    "\n",
    "    n_batch = train_samples // BATCH_SIZE # train samples is class variable\n",
    "\n",
    "    # parameters for saving and early stopping\n",
    "    saver = tf.train.Saver()\n",
    "    patience = hparams['patience']\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "   \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        best_acc = 0.0\n",
    "        DONE = False\n",
    "        epoch = 0\n",
    "        if load_model == True:\n",
    "            saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "            all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "            for v in all_vars:\n",
    "                v_ = sess.run(v)\n",
    "                print(v_)\n",
    "            print(\"Finished loading model\")\n",
    "\n",
    "        while epoch <= EPOCHS and not DONE:\n",
    "            loss = 0.0\n",
    "            batch = 1\n",
    "            epoch += 1\n",
    "\n",
    "#             with open(TRAIN_SET, 'r') as f:\n",
    "#                 # Load the file in a TextReader object until we've read it all\n",
    "#                 # then create a new TextReader Object for how many epochs we \n",
    "#                 # want to loop on\n",
    "#                 reader = TextReader(f, max_word_length)\n",
    "                \n",
    "            for minibatch in iterate_minibatch(BATCH_SIZE, X_train, Y_train):\n",
    "                batch_x, batch_y = minibatch\n",
    "\n",
    "                # Do backprop and compute the cost and accuracy on this minibatch\n",
    "                _, c, a = sess.run([optimizer, cost, acc],\n",
    "                                   feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "                loss += c\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    # Compute Accuracy on the Training set and print some info\n",
    "                    print('Epoch: %5d/%5d -- batch: %5d/%5d -- Loss: %.4f -- Train Accuracy: %.4f' %\n",
    "                          (epoch, EPOCHS, batch, n_batch, loss/batch, a))\n",
    "\n",
    "                    # Write loss and accuracy to some file\n",
    "                    log = open(LOGGING_PATH, 'a')\n",
    "                    log.write('%s, %6d, %.5f, %.5f \\n' % ('train', epoch * batch, loss/batch, a))\n",
    "                    log.close()\n",
    "\n",
    "                # --------------\n",
    "                # EARLY STOPPING\n",
    "                # --------------\n",
    "\n",
    "                # Compute Accuracy on the Validation set, check if validation has improved, save model, etc\n",
    "                if batch % 500 == 0:\n",
    "                    m = 1\n",
    "                    accuracy = []\n",
    "\n",
    "                    # Validation set is very large, so accuracy is computed on testing set\n",
    "                    # instead of valid set, change TEST_SET to VALID_SET to compute accuracy on valid set\n",
    "#                     with open(TEST_SET, 'r') as ff:\n",
    "#                         valid_reader = TextReader(ff, max_word_length)\n",
    "                    for mb in iterate_minibatch(BATCH_SIZE, X_test, Y_test):\n",
    "                        valid_x, valid_y = mb\n",
    "                        a = sess.run([acc], feed_dict={X: valid_x, Y: valid_y}) # X and Y are placeholder nodes\n",
    "                        accuracy.append(a)\n",
    "                        m +=1\n",
    "                        if m == 10: # to speed up training only go through 10 batches \n",
    "                            break\n",
    " \n",
    "                    mean_acc = np.mean(accuracy)\n",
    "\n",
    "                    # if accuracy has improved, save model and boost patience\n",
    "                    if mean_acc > best_acc:\n",
    "                        best_acc = mean_acc\n",
    "                        save_path = saver.save(sess, SAVE_PATH)\n",
    "                        patience = hparams['patience']\n",
    "                        print('Model saved in file: %s' % save_path)\n",
    "\n",
    "                    # else reduce patience and break loop if necessary\n",
    "                    else:\n",
    "                        patience -= 500\n",
    "                        if patience <= 0:\n",
    "                            DONE = True\n",
    "                            break\n",
    "\n",
    "                    print('Epoch: %5d/%5d -- batch: %5d/%5d -- Valid Accuracy: %.4f' %\n",
    "                         (epoch, EPOCHS, batch, n_batch, mean_acc))\n",
    "\n",
    "                    # Write validation accuracy to log file\n",
    "                    log = open(LOGGING_PATH, 'a')\n",
    "                    log.write('%s, %6d, %.5f \\n' % ('valid', epoch * batch, mean_acc))\n",
    "                    log.close()\n",
    "\n",
    "                batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tensorflow/lstm\n",
      "Finished loading model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-922868d3bc9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-547f8d86ad4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(hparams, X_train, Y_train, X_test, Y_test, load_model)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# Do backprop and compute the cost and accuracy on this minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 _, c, a = sess.run([optimizer, cost, acc],\n\u001b[0;32m---> 77\u001b[0;31m                                    feed_dict={X: batch_x, Y: batch_y})\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(lstm.get_hparams(),X_train, Y_train, X_test, Y_test, load_model=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_test_set(hparams, X_val, Y_val):\n",
    "        \"\"\"\n",
    "        Evaluate Test Set\n",
    "        On a model that trained for around 5 epochs it achieved:\n",
    "        # Valid loss: 23.50035 -- Valid Accuracy: 0.83613\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "        max_word_length = hparams['max_word_length']\n",
    "\n",
    "        pred = prediction\n",
    "        \n",
    "        cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "        acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "        # parameters for restoring variables\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            print('Loading model %s...' % SAVE_PATH)\n",
    "            saver.restore(sess, SAVE_PATH)\n",
    "            print('Done!')\n",
    "            loss = []\n",
    "            accuracy = []\n",
    "\n",
    "#             with open(VALID_SET, 'r') as f:\n",
    "#                 reader = TextReader(f, max_word_length)\n",
    "            for minibatch in iterate_minibatch(BATCH_SIZE, X_val, Y_vak):\n",
    "                batch_x, batch_y = minibatch\n",
    "\n",
    "                c, a = sess.run([cost, acc], feed_dict={X: batch_x, Y: batch_y}) # X and Y are placeholder variables\n",
    "                loss.append(c)\n",
    "                accuracy.append(a)\n",
    "\n",
    "            loss = np.mean(loss)\n",
    "            accuracy = np.mean(accuracy)\n",
    "            print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "            return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1 , l = encode_one_hot(X_all[1],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2 , l = encode_one_hot(X_all[2],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to train two models - LSTM and CNN to sentiment analysis\n",
    "- Using tweets here because the primary source of text from founders will be Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_founder",
   "language": "python",
   "name": "nlp_founder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
