{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import math as math\n",
    "import collections as collections\n",
    "import random as random\n",
    "from numpy.random import randint\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "# open twitter sentiment data\n",
    "# 1 = positive\n",
    "# 0 = Negative\n",
    "twitter_data_df = pd.read_csv(\"../data/raw/Twitter_Sentiment_Analysis/Sentiment Analysis Dataset.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = twitter_data_df.SentimentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_all = twitter_data_df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578612, 4)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t, X_val, Y_t, Y_val = train_test_split(X_all,Y_all, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_t,Y_t, test_size=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126289,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once this is done, we need to convert the sentences as a one-hot tensor of shape [sentence_length x word_length x alphabet_size].\n",
    "- https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMB_ALPHABET = 'abcdefghijklmnopqrstuvwxyz0123456789-,;' \\\n",
    "               '.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "MAX_WORD_LENGTH = 16 # number of characters in a word\n",
    "ALPHABET_SIZE = len(EMB_ALPHABET)\n",
    "# we associate every character in our alphabet to a number: \n",
    "# e.g. b => 1 d => 3 etc.\n",
    "ALPHABET_DICT = {ch: ix for ix, ch in enumerate(EMB_ALPHABET)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_one_hot( sentence, emb_alphabet, max_word_length, alphabet_size, alphabet_dict):\n",
    "    \"\"\"Convert a sentence to a one hot character encoding tensor for that sentence using alphanumeric characters\"\"\"\n",
    "    # https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/\n",
    "    # Convert Sentences to np.array of Shape \n",
    "    # ('sent_length', 'word_length', 'emb_size')\n",
    "\n",
    "    sent = []\n",
    "\n",
    "    # We need to keep track of the maximum length of the sentence in a minibatch\n",
    "    # so that we can pad them with zeros, this is why we return the length of every\n",
    "    # sentences after they are converted to one-hot tensors\n",
    "    SENT_LENGTH = 0\n",
    "\n",
    "    # Here, we remove any non-printable characters in a sentence (mostly\n",
    "    # non-ASCII characters)\n",
    "    printable = emb_alphabet\n",
    "    encoded_sentence = filter(lambda x: x in printable, sentence)\n",
    "\n",
    "    # word_tokenize() splits a sentence into an array where each element is\n",
    "    # a word in the sentence, for example, \n",
    "    # \"My name is Charles\" => [\"My\", \"name\", \"is\", Charles\"]\n",
    "    blob = TextBlob(sentence)\n",
    "    individual_words_from_sentence = blob.tokenize()\n",
    "    for word in individual_words_from_sentence :\n",
    "\n",
    "        # Encode one word as a matrix of shape [max_word_length x ALPHABET_SIZE]\n",
    "        word_encoding = np.zeros(shape=(max_word_length, alphabet_size))\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "\n",
    "            # If the character is not in the alphabet, ignore it    \n",
    "            try:\n",
    "                char_encoding = alphabet_dict[char]\n",
    "                one_hot = np.zeros(ALPHABET_SIZE)\n",
    "                one_hot[char_encoding] = 1\n",
    "                word_encoding[i] = one_hot\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        sent.append(np.array(word_encoding))\n",
    "        SENT_LENGTH += 1\n",
    "\n",
    "    return np.array(sent), SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array,length = encode_one_hot(X_all[1], EMB_ALPHABET , MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT)\n",
    "# array is number of words in sentence X Max word length  X one hot encoding for alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad tensors to all have the same size \n",
    "- [batch_size x maximum_sentence_length x maximum_word_length x alphabet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   is so sad for my APL friend............'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[0][2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_minibatch( sentences, sentiment_y,  max_word_length, alphabet_size):\n",
    "    \"\"\"Create a minibath of one-hot encoded sentences \n",
    "    array is Batch Size X max number of words in sentence X Max word length  X one hot encoding for alphabet,\n",
    "    \n",
    "    and one hot encoded y [0,1] or [1,0] whre 1=positive\"\"\"\n",
    "    # Create a minibatch of sentences and convert sentiment\n",
    "    # to a one-hot vector, also takes care of padding\n",
    "\n",
    "    max_word_length = max_word_length\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    max_length = 0\n",
    "\n",
    "    for sentence,sent in zip(sentences, sentiment_y):\n",
    "        # Append the one-hot encoding of the sentiment to the minibatch of Y\n",
    "        # 0: Negative 1: Positive\n",
    "        minibatch_y.append(np.array([0, 1]) if sent == 0 else np.array([1, 0]))\n",
    "\n",
    "        # One-hot encoding of the sentence\n",
    "        one_hot, length = encode_one_hot(sentence,EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )\n",
    "\n",
    "        # Calculate maximum_sentence_length\n",
    "        if length >= max_length:\n",
    "            max_length = length\n",
    "\n",
    "        # Append encoded sentence to the minibatch of X\n",
    "        minibatch_x.append(one_hot)\n",
    "\n",
    "\n",
    "    # data is a np.array of shape ('b', 's', 'w', 'e') we want to\n",
    "    # pad it with np.zeros of shape ('e',) to get \n",
    "    # ('b', 'SENTENCE_MAX_LENGTH', 'WORD_MAX_LENGTH', 'e')\n",
    "    def numpy_fillna(data):\n",
    "        \"\"\" This is a very useful function that fill the holes in our tensor \"\"\"\n",
    "\n",
    "        # Get lengths of each row of data\n",
    "        lens = np.array([len(i) for i in data])\n",
    "\n",
    "        # Mask of valid places in each row\n",
    "        mask = np.arange(lens.max()) < lens[:, None]\n",
    "        #print(mask)\n",
    "\n",
    "        # Setup output array and put elements from data into masked positions\n",
    "        out = np.zeros(shape=(mask.shape + (max_word_length, alphabet_size)),\n",
    "                       dtype='float32')\n",
    "        #print(out)\n",
    "\n",
    "        out[mask] = np.concatenate(data)\n",
    "        #print(out,'final')\n",
    "        return out\n",
    "\n",
    "    # Padding...\n",
    "    minibatch_x = numpy_fillna(minibatch_x)\n",
    "\n",
    "    return minibatch_x, np.array(minibatch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x, minibatch_y = make_minibatch(X_test[:15],Y_test[:15], MAX_WORD_LENGTH, ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 36, 16, 69)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a generator to feed to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatch(batch_size, X_data, y_data):\n",
    "        \"\"\" Returns Next Batch \"\"\"\n",
    "        \n",
    "        n_samples = len(X_data)\n",
    "        \n",
    "        # Number of batches / number of iterations per epoch\n",
    "        n_batch = int(n_samples // batch_size)\n",
    "        \n",
    "        # Creates a minibatch, loads it to RAM and feed it to the network\n",
    "        # until the buffer is empty\n",
    "        for i in range(n_batch):\n",
    "\n",
    "            inputs, targets = make_minibatch(X_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             y_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             MAX_WORD_LENGTH, ALPHABET_SIZE)\n",
    "            yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "    \"\"\" Straight-forward convvolutional layer \"\"\"\n",
    "    # w is the kernel, b the bias, no strides and VALID padding\n",
    "    \n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "            b = tf.get_variable('b', [output_dim])\n",
    "\n",
    "        return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tdnn(input_, kernels, kernel_features, ALPHABET_SIZE, scope='TDNN'):\n",
    "    ''' Time Delay Neural Network\n",
    "    :input:           input float tensor of shape \n",
    "                      [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "    :kernels:         array of kernel sizes\n",
    "    :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "    '''\n",
    "    assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "\n",
    "    # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'embed_size') we\n",
    "    # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
    "    # use conv2D\n",
    "    # It might not seem obvious why we need to use this small hack at first sight, the reason\n",
    "    # is that sentence_length will change across the different minibatches, but if we kept it\n",
    "    # as is sentence_length would act as the number of channels in the convnet which NEEDS to\n",
    "    # stay the same\n",
    "    input_ = tf.reshape(input_, [-1, MAX_WORD_LENGTH, ALPHABET_SIZE])\n",
    "    input_ = tf.expand_dims(input_, 1)\n",
    "\n",
    "    layers = []\n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "        with tf.variable_scope(scope):\n",
    "            for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "                reduced_length = MAX_WORD_LENGTH - kernel_size + 1\n",
    "\n",
    "                # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
    "                conv = conv2d(input_, kernel_feature_size, 1,\n",
    "                              kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "\n",
    "                # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
    "                pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "                layers.append(tf.squeeze(pool, [1, 2]))\n",
    "\n",
    "            if len(kernels) > 1:\n",
    "                output = tf.concat(layers, 1)\n",
    "            else:\n",
    "                output = layers[0]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This goes in ops.py\n",
    "\n",
    "def linear(input_, output_size, scope=None):\n",
    "    \"\"\"\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "    \n",
    "    Args:\n",
    "        args: a tensor or a list of 2D, batch x n, Tensors.\n",
    "        output_size: int, second dimension of W[i].\n",
    "        scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "        \n",
    "    Returns:\n",
    "        A 2D Tensor with shape [batch x output_size] equal to\n",
    "        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        # Now the computation.\n",
    "        with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "            matrix = tf.get_variable(\"Matrix\", [output_size, input_size],\n",
    "                                     dtype=input_.dtype)\n",
    "            bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway network to help train deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            for idx in range(num_layers):\n",
    "                g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "                t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "                output = t * g + (1. - t) * input_\n",
    "                input_ = output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(object):\n",
    "    \"\"\" Character-Level LSTM Implementation \"\"\"\n",
    "\n",
    "    def __init__(self, hparams=None):\n",
    "        # Get the hyperparameters\n",
    "        self.hparams = self.get_hparams()\n",
    "        g = tf.get_default_graph()\n",
    "        with g.as_default():\n",
    "        \n",
    "            # maximum length of each words\n",
    "            max_word_length = self.hparams['max_word_length']\n",
    "\n",
    "            # X is of shape ('b', 'sentence_length', 'max_word_length', 'alphabet_size']\n",
    "            # Placeholder for the one-hot encoded sentences\n",
    "            X = tf.placeholder('float32', \n",
    "                                    shape=[None, None, max_word_length, ALPHABET_SIZE],\n",
    "                                    name='X')\n",
    "            self.X = X\n",
    "\n",
    "            # Placeholder for the one-hot encoded sentiment\n",
    "            Y = tf.placeholder('float32', shape=[None, 2], name='Y')\n",
    "            self.Y = Y\n",
    "        \n",
    "    def get_hparams(self):\n",
    "        ''' Get Hyperparameters '''\n",
    "\n",
    "        return {\n",
    "            'BATCH_SIZE':       64,\n",
    "            'EPOCHS':           500,\n",
    "            'max_word_length':  16,\n",
    "            'learning_rate':    0.0001,\n",
    "            'patience':         10000,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = lstm.X # tf placeholder \n",
    "Y = lstm.Y # tf placeholder for class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input_, out_dim, scope=None):\n",
    "    \"\"\" SoftMax Output \"\"\"\n",
    "    \n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.variable_scope(scope or 'softmax'):\n",
    "            W = tf.get_variable('W', [input_.get_shape()[1], out_dim])\n",
    "            b = tf.get_variable('b', [out_dim])\n",
    "\n",
    "    return tf.nn.softmax(tf.matmul(input_, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126289,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = None # class variable\n",
    "train_samples=1136600\n",
    "valid_samples = 126289\n",
    "def build(\n",
    "              training=True,\n",
    "              testing_batch_size=1000,\n",
    "              kernels=[1, 2, 3, 4, 5, 6, 7],\n",
    "              kernel_features=[25, 50, 75, 100, 125, 150, 175],\n",
    "              rnn_size=650,\n",
    "              dropout=0.0,\n",
    "              size=700,\n",
    "              train_samples=train_samples,\n",
    "              valid_samples=valid_samples,\n",
    "              hparams= lstm.get_hparams()):\n",
    "        \"\"\"\n",
    "        Build the computational graph\n",
    "        \n",
    "        :param training: \n",
    "            Boolean whether we are training (True) or testing (False)\n",
    "        \n",
    "        :param testing_batch_size: \n",
    "            Batch size to use during testing \n",
    "            \n",
    "        :param kernels:\n",
    "            Kernel width for each convolutional layer\n",
    "         \n",
    "        :param kernel_features: \n",
    "            Number of kernels for each convolutional layer\n",
    "            \n",
    "        :param rnn_size: \n",
    "            Size of the LSTM output\n",
    "        \n",
    "        :param dropout: \n",
    "            Retain probability when using dropout\n",
    "        \n",
    "        :param size: \n",
    "            Size of the Highway embeding\n",
    "        \n",
    "        :param train_samples: \n",
    "            Number of training samples\n",
    "        \n",
    "        :param valid_samples: \n",
    "            Number of validation samples\n",
    "        :hyperparameters\n",
    "            dict of parameters\n",
    "        \"\"\"\n",
    "\n",
    "        max_word_length = hparams['max_word_length']\n",
    "        size = size\n",
    "        max_word_length = hparams['max_word_length']\n",
    "        train_samples = train_samples\n",
    "        valid_samples = valid_samples\n",
    "\n",
    "        \n",
    "        # If we are training use the BATCH_SIZE from the hyperparameters\n",
    "        # else use the testing batch size\n",
    "        if training == True:\n",
    "            BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "\n",
    "        else:\n",
    "            BATCH_SIZE = testing_batch_size\n",
    "            \n",
    "       \n",
    "\n",
    "        \n",
    "        # Pass the sentences through the CharCNN network\n",
    "        cnn = tdnn(X, kernels, kernel_features, ALPHABET_SIZE) # X is a TF placeholder from the LSTM network\n",
    "\n",
    "        # tdnn() returns a tensor of shape [batch_size * sentence_length x kernel_features]\n",
    "        # highway() returns a tensor of shape [batch_size * sentence_length x size] to use\n",
    "        # tensorflow dynamic_rnn module we need to reshape it to \n",
    "        # [batch_size x sentence_length x size]\n",
    "        cnn = highway(cnn, size)\n",
    "\n",
    "        cnn = tf.reshape(cnn, [BATCH_SIZE, -1, size])\n",
    "        # deafult graph\n",
    "        g = tf.get_default_graph()\n",
    "        with g.as_default():\n",
    "        \n",
    "            # Build the LSTM\n",
    "            with tf.variable_scope('LSTM'):\n",
    "\n",
    "                # The following is pretty straight-forward, create a cell and add dropout if\n",
    "                # necessary. Note that I did not use dropout to get my results, but using it\n",
    "                # will probably help\n",
    "                def create_rnn_cell():\n",
    "                    cell = rnn.BasicLSTMCell(rnn_size, state_is_tuple=True,\n",
    "                                             forget_bias=0.0, reuse=False)\n",
    "\n",
    "                    if dropout > 0.0:\n",
    "                        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1. - dropout)\n",
    "\n",
    "                    return cell\n",
    "\n",
    "                cell = create_rnn_cell()\n",
    "\n",
    "                # Initial state of the LSTM cell\n",
    "                initial_rnn_state = cell.zero_state(BATCH_SIZE, dtype='float32')\n",
    "\n",
    "                # This function returns the outputs at every steps of \n",
    "                # the LSTM (i.e. one output for every word)\n",
    "                outputs, final_rnn_state = tf.nn.dynamic_rnn(cell, cnn,\n",
    "                                                             initial_state=initial_rnn_state,\n",
    "                                                             dtype=tf.float32)\n",
    "\n",
    "\n",
    "                # In this implementation, we only care about the last outputs of the RNN\n",
    "                # i.e. the output at the end of the sentence\n",
    "                outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "                last = outputs[-1]\n",
    "            global prediction\n",
    "            prediction = softmax(last, 2)\n",
    "        \n",
    "\n",
    "\n",
    "size = None\n",
    "hparams = None\n",
    "max_word_length = None\n",
    "\n",
    "\n",
    "build(hparams=lstm.get_hparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(64, 2) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PATH = '/NOBACKUP/ashbylepoc/PycharmProjects/CharLSTM/'\n",
    "# TRAIN_SET = PATH + 'datasets/train_set.csv'\n",
    "# TEST_SET = PATH + 'datasets/test_set.csv'\n",
    "# VALID_SET = PATH + 'datasets/valid_set.csv'\n",
    "\n",
    "SAVE_PATH = 'tensorflow/lstm'\n",
    "LOGGING_PATH = 'tensorflow/log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(hparams, X_train, Y_train, X_test, Y_test, load_model = False):\n",
    "    \n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "    EPOCHS = hparams['EPOCHS']\n",
    "    max_word_length = hparams['max_word_length']\n",
    "    learning_rate = hparams['learning_rate']\n",
    "\n",
    "    # the probability for each sentiment (pos, neg)\n",
    "    pred = prediction\n",
    "\n",
    "    # Binary cross-entropy loss\n",
    "    cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "    # The number of \"predictions\" we got right, we assign a sentence with \n",
    "    # a positive connotation when the probability to be positive is greater then\n",
    "    # the probability of being negative and vice-versa.\n",
    "    predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    # Accuracy: # predictions right / total number of predictions\n",
    "    acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "    # We use the Adam Optimizer\n",
    "    \n",
    "    try: # see if this optimizer is already defined\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cost)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "    n_batch = train_samples // BATCH_SIZE # train samples is class variable\n",
    "\n",
    "    # parameters for saving and early stopping\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    patience = hparams['patience']\n",
    "    \n",
    "    \n",
    "   # tf.add_to_collection('train_op', train_op) ## for the variables to be restores\n",
    "    \n",
    "    # get graph\n",
    "    g = tf.get_default_graph()\n",
    "\n",
    "\n",
    "    with tf.Session(graph = g) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        best_acc = 0.0\n",
    "        DONE = False\n",
    "        epoch = 0\n",
    "        g_step = 0\n",
    "        if load_model == True:\n",
    "            saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "            all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "            for v in all_vars:\n",
    "                v_ = sess.run(v)\n",
    "                print(v_)\n",
    "            print(\"Finished loading model\")\n",
    "\n",
    "        while epoch <= EPOCHS and not DONE:\n",
    "            loss = 0.0\n",
    "            batch = 1\n",
    "            epoch += 1\n",
    "\n",
    "#             with open(TRAIN_SET, 'r') as f:\n",
    "#                 # Load the file in a TextReader object until we've read it all\n",
    "#                 # then create a new TextReader Object for how many epochs we \n",
    "#                 # want to loop on\n",
    "#                 reader = TextReader(f, max_word_length)\n",
    "                \n",
    "            for minibatch in iterate_minibatch(BATCH_SIZE, X_train, Y_train):\n",
    "                g_step +=1\n",
    "                batch_x, batch_y = minibatch\n",
    "\n",
    "                # Do backprop and compute the cost and accuracy on this minibatch\n",
    "                _, c, a = sess.run([optimizer, cost, acc],\n",
    "                                   feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "                loss += c\n",
    "\n",
    "                if batch % 10 == 0: # change back to 100\n",
    "                    # Compute Accuracy on the Training set and print some info\n",
    "                    print('Epoch: %5d/%5d -- batch: %5d/%5d -- Loss: %.4f -- Train Accuracy: %.4f -- Patience %.4f' %\n",
    "                          (epoch, EPOCHS, batch, n_batch, loss/batch, a, patience))\n",
    "\n",
    "                    # Write loss and accuracy to some file\n",
    "                    log = open(LOGGING_PATH, 'a')\n",
    "                    log.write('%s, %6d, %.5f, %.5f \\n' % ('train', epoch * batch, loss/batch, a))\n",
    "                    log.close()\n",
    "                    \n",
    "\n",
    "                # --------------\n",
    "                # EARLY STOPPING\n",
    "                # --------------\n",
    "\n",
    "                # Compute Accuracy on the Validation set, check if validation has improved, save model, etc\n",
    "                if batch % 500 == 0:\n",
    "                    m = 1\n",
    "                    accuracy = []\n",
    "\n",
    "                    # Validation set is very large, so accuracy is computed on testing set\n",
    "                    # instead of valid set, change TEST_SET to VALID_SET to compute accuracy on valid set\n",
    "#                     with open(TEST_SET, 'r') as ff:\n",
    "#                         valid_reader = TextReader(ff, max_word_length)\n",
    "                    for mb in iterate_minibatch(BATCH_SIZE, X_test, Y_test):\n",
    "                        valid_x, valid_y = mb\n",
    "                        a = sess.run([acc], feed_dict={X: valid_x, Y: valid_y}) # X and Y are placeholder nodes\n",
    "                        accuracy.append(a)\n",
    "                        m +=1\n",
    "                        if m == 10: # to speed up training only go through 10 batches \n",
    "                            break\n",
    " \n",
    "                    mean_acc = np.mean(accuracy)\n",
    "\n",
    "                    # if accuracy has improved, save model and boost patience\n",
    "                    if mean_acc > best_acc:\n",
    "                        best_acc = mean_acc\n",
    "                        save_path = saver.save(sess, SAVE_PATH, global_step=g_step)\n",
    "                        patience = hparams['patience']\n",
    "                        print('Model saved in file: %s' % save_path)\n",
    "\n",
    "                    # else reduce patience and break loop if necessary\n",
    "                    else:\n",
    "                        patience -= 500\n",
    "                        if patience <= 0:\n",
    "                            DONE = True\n",
    "                            break\n",
    "\n",
    "                    print('Epoch: %5d/%5d -- batch: %5d/%5d -- Valid Accuracy: %.4f' %\n",
    "                         (epoch, EPOCHS, batch, n_batch, mean_acc))\n",
    "\n",
    "                    # Write validation accuracy to log file\n",
    "                    log = open(LOGGING_PATH, 'a')\n",
    "                    log.write('%s, %6d, %.5f \\n' % ('valid', epoch * batch, mean_acc))\n",
    "                    log.close()\n",
    "\n",
    "                batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(lstm.get_hparams(),X_train, Y_train, X_test, Y_test, load_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_test_set(hparams, X_val, Y_val):\n",
    "        \"\"\"\n",
    "        Evaluate Test Set\n",
    "        On a model that trained for around 5 epochs it achieved:\n",
    "        # Valid loss: 23.50035 -- Valid Accuracy: 0.83613\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "        max_word_length = hparams['max_word_length']\n",
    "\n",
    "        pred = prediction\n",
    "        \n",
    "        cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "        acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "        # restart the graph\n",
    "        tf.reset_default_graph()\n",
    "        # parameters for restoring variables\n",
    "        \n",
    "    \n",
    "        #saver = tf.train.Saver()\n",
    "        #saver = tf.train.import_meta_graph('./tensorflow/lstm.meta')\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            print('Loading model %s...' % SAVE_PATH)\n",
    "            saver = tf.train.import_meta_graph('./tensorflow/lstm.meta')\n",
    "            var = tf.global_variables()[0]\n",
    "            print(var)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            \n",
    "            saver.restore(sess, SAVE_PATH)\n",
    "            print('Done!')\n",
    "            loss = []\n",
    "            accuracy = []\n",
    "\n",
    "#             with open(VALID_SET, 'r') as f:\n",
    "#                 reader = TextReader(f, max_word_length)\n",
    "            for minibatch in iterate_minibatch(BATCH_SIZE, X_val, Y_val):\n",
    "                batch_x, batch_y = minibatch\n",
    "\n",
    "                c, a = sess.run([cost, acc], feed_dict={X: batch_x, Y: batch_y}) # X and Y are placeholder variables\n",
    "                loss.append(c)\n",
    "                accuracy.append(a)\n",
    "\n",
    "            loss = np.mean(loss)\n",
    "            accuracy = np.mean(accuracy)\n",
    "            print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "            return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_test_set(lstm.get_hparams(),X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tensorflow/lstm\n",
      "Finished loading model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        best_acc = 0.0\n",
    "        DONE = False\n",
    "        epoch = 0\n",
    "        g_step = 0\n",
    "        #if load_model == True:\n",
    "        saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "        all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "        for v in all_vars:\n",
    "            v_ = sess.run(v)\n",
    "            print(v_)\n",
    "        print(\"Finished loading model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model tensorflow/lstm...\n",
      "INFO:tensorflow:Restoring parameters from ./tensorflow/lstm\n",
      "Done!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_5:0\", shape=(64, 2), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-4f7d979b08e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keep_dims, name, reduction_indices)\u001b[0m\n\u001b[1;32m   1234\u001b[0m       \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, reduction_indices, keep_dims, name)\u001b[0m\n\u001b[1;32m   2654\u001b[0m   result = _op_def_lib.apply_op(\"Sum\", input=input,\n\u001b[1;32m   2655\u001b[0m                                 \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m                                 keep_dims=keep_dims, name=name)\n\u001b[0m\u001b[1;32m   2657\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m       \u001b[0;31m# pyline: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3849\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3851\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_5:0\", shape=(64, 2), dtype=float32)."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('Loading model %s...' % SAVE_PATH)\n",
    "    saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "    all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "    for v in all_vars:\n",
    "        v_ = sess.run(v)\n",
    "        print(v_,'Var')\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    pred = prediction\n",
    "\n",
    "\n",
    "    cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "    predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "        \n",
    "    loss = []\n",
    "    accuracy = []\n",
    "        \n",
    "    for minibatch in iterate_minibatch(BATCH_SIZE, X_val, Y_val):\n",
    "            batch_x, batch_y = minibatch\n",
    "\n",
    "            c, a = sess.run([cost, acc], feed_dict={X: batch_x, Y: batch_y}) # X and Y are placeholder variables\n",
    "            loss.append(c)\n",
    "            accuracy.append(a)\n",
    "\n",
    "    loss = np.mean(loss)\n",
    "    accuracy = np.mean(accuracy)\n",
    "    print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "    print(loss, accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model tensorflow/lstm...\n",
      "INFO:tensorflow:Restoring parameters from ./tensorflow/lstm\n",
      "Finished loading model\n",
      "Done!\n",
      "<tensorflow.python.framework.ops.Graph object at 0x11a6441d0> graph\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_7:0\", shape=(64, 2), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ef6f2b006261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keep_dims, name, reduction_indices)\u001b[0m\n\u001b[1;32m   1234\u001b[0m       \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, reduction_indices, keep_dims, name)\u001b[0m\n\u001b[1;32m   2654\u001b[0m   result = _op_def_lib.apply_op(\"Sum\", input=input,\n\u001b[1;32m   2655\u001b[0m                                 \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m                                 keep_dims=keep_dims, name=name)\n\u001b[0m\u001b[1;32m   2657\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m       \u001b[0;31m# pyline: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3849\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3851\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_7:0\", shape=(64, 2), dtype=float32)."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE =64 #hparams['BATCH_SIZE']\n",
    "max_word_length = 16#hparams['max_word_length']\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('Loading model %s...' % SAVE_PATH)\n",
    "    saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "    all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "    for v in all_vars:\n",
    "        v_ = sess.run(v)\n",
    "        print(v_,'Var')\n",
    "    print(\"Finished loading model\")\n",
    "    print('Done!')\n",
    "    g = tf.get_default_graph()\n",
    "    print(g,'graph')\n",
    "    with g.as_default():\n",
    "\n",
    "        pred = prediction\n",
    "\n",
    "        cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "        acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "        # # restart the graph\n",
    "        # tf.reset_default_graph()\n",
    "        # parameters for restoring variables\n",
    "\n",
    "        #saver = tf.train.Saver()\n",
    "        #saver = tf.train.import_meta_graph('./tensorflow/lstm.meta')\n",
    "\n",
    "\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "\n",
    "        #             with open(VALID_SET, 'r') as f:\n",
    "        #                 reader = TextReader(f, max_word_length)\n",
    "        for minibatch in iterate_minibatch(BATCH_SIZE, X_val, Y_val):\n",
    "            batch_x, batch_y = minibatch\n",
    "\n",
    "            c, a = sess.run([cost, acc], feed_dict={X: batch_x, Y: batch_y}) # X and Y are placeholder variables\n",
    "            loss.append(c)\n",
    "            accuracy.append(a)\n",
    "\n",
    "        loss = np.mean(loss)\n",
    "        accuracy = np.mean(accuracy)\n",
    "        print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f8caf16cb8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1 , l = encode_one_hot(X_all[1],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2 , l = encode_one_hot(X_all[2],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to train two models - LSTM and CNN to sentiment analysis\n",
    "- Using tweets here because the primary source of text from founders will be Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try creating a toy model\n",
    "- save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Attempt\n",
    "- Use multilayer perceptron  with count vectorizer to predict sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                     is so sad for my APL friend.............'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downsample\n",
    "\n",
    "X_down = X_all[:80_000]\n",
    "Y_down = Y_all[:80_000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_down = [[1,0] if i == 1 else [0,1] for i in Y_down]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_features = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=m_features)\n",
    "# try also TFIDF\n",
    "tfidf_vec = TfidfVectorizer(max_features=m_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_cvectors = count_vec.fit_transform(X_down)\n",
    "X_cvectors = tfidf_vec.fit_transform(X_down)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dense =  np.array(X_cvectors.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, Y_down, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.random.shuffle(X_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, X, Y, shuffle=True):\n",
    "    \"\"\"Return a generator of batches for X and Y data\"\"\"\n",
    "    # shuffle the examples\n",
    "    if shuffle:\n",
    "        np.random.shuffle(X)\n",
    "        np.random.shuffle(Y)\n",
    "    \n",
    "    for i in range(0,len(X),batch_size):\n",
    "        yield X[i:i+batch_size], np.array(Y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = next(batch_generator(16, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 10000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron for sentiment analysis first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 200\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "EMB_ALPHABET = 'abcdefghijklmnopqrstuvwxyz0123456789-,;' \\\n",
    "               '.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "MAX_WORD_LENGTH = 16 # number of characters in a word\n",
    "ALPHABET_SIZE = len(EMB_ALPHABET)\n",
    "# we associate every character in our alphabet to a number: \n",
    "# e.g. b => 1 d => 3 etc.\n",
    "ALPHABET_DICT = {ch: ix for ix, ch in enumerate(EMB_ALPHABET)}\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_hidden_3 = 256\n",
    "n_input = m_features # max features\n",
    "n_classes = 2 # sentiment (0 = negative, 1 = positive)\n",
    "\n",
    "\n",
    "\n",
    "# save path\n",
    "SAVE_PATH = 'tensorflow/MLP_sentiment_tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # hidden 3\n",
    "    layer_3 = tf.add(tf.matmul(layer_1, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    #output\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "\n",
    "#### maybe change the loss here to binary cross entropy \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    " # Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 5.768409729\n",
      "Average cost = 30.5418810027\n",
      "Accuracy: 0.75\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0011 cost= 1.405854344\n",
      "Average cost = 1.47460775756\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0021 cost= 0.647772312\n",
      "Average cost = 0.80973352062\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0031 cost= 0.667012453\n",
      "Average cost = 0.693808201796\n",
      "Accuracy: 0.5\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0041 cost= 0.652975202\n",
      "Average cost = 0.692506928283\n",
      "Accuracy: 0.375\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0051 cost= 0.710689425\n",
      "Average cost = 0.689293405566\n",
      "Accuracy: 0.875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0061 cost= 0.669481099\n",
      "Average cost = 0.688162672596\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0071 cost= 0.712082744\n",
      "Average cost = 0.688471787824\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0081 cost= 0.671971917\n",
      "Average cost = 0.686496698895\n",
      "Accuracy: 0.5\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0091 cost= 0.675508142\n",
      "Average cost = 0.686600891252\n",
      "Accuracy: 0.5\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0101 cost= 0.686495781\n",
      "Average cost = 0.68637110484\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0111 cost= 0.669466257\n",
      "Average cost = 0.688916463691\n",
      "Accuracy: 0.4375\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0121 cost= 0.752575219\n",
      "Average cost = 0.68708189596\n",
      "Accuracy: 0.75\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0131 cost= 0.696829855\n",
      "Average cost = 0.688783413516\n",
      "Accuracy: 0.4375\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0141 cost= 0.686438262\n",
      "Average cost = 0.685887538279\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0151 cost= 0.745445490\n",
      "Average cost = 0.68884609697\n",
      "Accuracy: 0.3125\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0161 cost= 0.708082795\n",
      "Average cost = 0.686275991683\n",
      "Accuracy: 0.625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0171 cost= 0.697472155\n",
      "Average cost = 0.687412889149\n",
      "Accuracy: 0.4375\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0181 cost= 0.693699718\n",
      "Average cost = 0.68690380817\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0191 cost= 0.674385190\n",
      "Average cost = 0.68905353699\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    " # Initial training session\n",
    "    \n",
    "    ### DON\"T LAUNCH\n",
    "# Launch the graph\n",
    "# with tf.Session() as sess:\n",
    "#     # Initializing the variables\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     # accuracy calculation\n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     acc = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))\n",
    "    \n",
    "\n",
    "#     # Training cycle\n",
    "#     for epoch in range(training_epochs):\n",
    "#         # set up the training generators\n",
    "#         gen_train = batch_generator(16, X_train, y_train)\n",
    "#         gen_test = batch_generator(16,X_test, y_test)\n",
    "    \n",
    "#         avg_cost = 0.\n",
    "#         total_batch = int(len(X_train)/batch_size)\n",
    "      \n",
    "\n",
    "        \n",
    "#         # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x, batch_y = next(gen_train)\n",
    "#             test_x, test_y = next(gen_test)\n",
    "#             # Run optimization op (backprop) and cost op (to get loss value)\n",
    "#             _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "#                                                           y: batch_y})\n",
    "            \n",
    "#             #a = sess.run([acc], feed_dict={x:test_x, y:test_y})\n",
    "#             # Compute average loss\n",
    "#             avg_cost += c / total_batch\n",
    "#         # Display logs per epoch step\n",
    "#         if epoch % display_step == 0:\n",
    "#             print( \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "#                 \"{:.9f}\".format(c))\n",
    "#             print(\"Average cost =\",avg_cost)\n",
    "#             print(\"Accuracy:\", acc.eval({x: test_x, y: test_y}))\n",
    "\n",
    "#              #Save model weights to disk\n",
    "#             saver = tf.train.Saver()\n",
    "#             save_path = saver.save(sess=sess, save_path = SAVE_PATH)\n",
    "#             print(\"Model saved in file: %s\" % SAVE_PATH)\n",
    "            \n",
    "#     print(\"Optimization Finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from tensorflow/MLP_sentiment_tfidf\n",
      "Model restored from file: tensorflow/MLP_sentiment_tfidf\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1136596",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-f6774b2b1c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Loop over all batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-bfb2d93e5e42>\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(batch_size, X, Y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Return a generator of batches for X and Y data\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# shuffle the xamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.shuffle (numpy/random/mtrand/mtrand.c:39578)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.shuffle (numpy/random/mtrand/mtrand.c:39528)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2477\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2478\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4404)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4087)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:14031)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:13975)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1136596"
     ]
    }
   ],
   "source": [
    "# Load an existing model\n",
    "tf.reset_default_graph()\n",
    "list_of_acc = []\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "    \n",
    "    \n",
    "    ## Create the graph\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "   # Store layers weight & bias\n",
    "    weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "\n",
    "    #### maybe change the loss here to binary cross entropy \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver = tf.train.Saver()\n",
    "    load_path = saver.restore(sess, SAVE_PATH)\n",
    "    print(\"Model restored from file: %s\" % SAVE_PATH)\n",
    "\n",
    "    # accuracy calculation\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # set up the training generators\n",
    "        gen_train = batch_generator(16, X_train, y_train)\n",
    "        gen_test = batch_generator(16,X_test, y_test)\n",
    "    \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(X_train)/batch_size)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next(gen_train)\n",
    "            test_x, test_y = next(gen_test)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            \n",
    "            #a = sess.run([acc], feed_dict={x:test_x, y:test_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print( \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(c))\n",
    "            print(\"Average cost =\",avg_cost)\n",
    "            print(\"Accuracy:\", acc.eval({x: test_x, y: test_y}))\n",
    "            list_of_acc.append(acc.eval({x: test_x, y: test_y}))\n",
    "\n",
    "             #Save model weights to disk\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess=sess, save_path = SAVE_PATH)\n",
    "            print(\"Model saved in file: %s\" % SAVE_PATH)\n",
    "            \n",
    "            \n",
    "    print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_lstm = [[1,0] if i == 1 else [0,1] for i in Y_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_lstm = X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lstm,  X_test_lstm,y_train_lstm, y_test_lstm = train_test_split(X_lstm,Y_lstm, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, define the embedding matrix\n",
    "- https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "  \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['wonderful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22856449\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in X_all: # add words to str\n",
    "    for word in tweet.split(' '):\n",
    "        data_set_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@alyankovic', 'RE:', 'No', 'pic', '-', 'on', 'my', 'computer', 'is', 'just']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_words [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_build_dataset, count, dictionary, reverse_dictionary = build_dataset(data_set_words,vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data [11622, 10591, 187, 543, 34, 15, 6, 589, 9, 26] ['@alyankovic', 'RE:', 'No', 'pic', '-', 'on', 'my', 'computer', 'is', 'just']\n"
     ]
    }
   ],
   "source": [
    "print('Sample data', data_build_dataset[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch_skip_gram(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    data_index = (data_index + 1) % len(data_build_dataset)\n",
    "    buffer.append(data_build_dataset[data_index])\n",
    "    \n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [skip_window]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data_build_dataset[data_index])\n",
    "    data_index = (data_index + 1) % len(data_build_dataset)\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data_build_dataset) - span) % len(data_build_dataset)\n",
    "  return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 to -> 37 your\n",
      "2 to -> 115 come\n",
      "37 your -> 2 to\n",
      "37 your -> 37 your\n",
      "37 your -> 37 your\n",
      "37 your -> 5768 knees\n",
      "5768 knees -> 37 your\n",
      "5768 knees -> 1 \n"
     ]
    }
   ],
   "source": [
    "batch_, labels_ = generate_batch_skip_gram(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch_[i], reverse_dictionary[batch_[i]],\n",
    "        '->', labels_[i, 0], reverse_dictionary[labels_[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['she']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she'"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37],\n",
       "       [ 115],\n",
       "       [   2],\n",
       "       [  37],\n",
       "       [  37],\n",
       "       [5768],\n",
       "       [  37],\n",
       "       [   1]], dtype=int32)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "max_tweet_length = 20 # Max number of words for a tweet\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector. Number of words per tweet\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(\n",
    "  tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for inputs\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the NCE loss, using a sample of the negative labels each time.\n",
    "loss = tf.reduce_mean(\n",
    "  tf.nn.nce_loss(weights=nce_weights,\n",
    "                 biases=nce_biases,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embed,\n",
    "                 num_sampled=num_sampled,\n",
    "                 num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                 biases=nce_biases,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embed,\n",
    "                 num_sampled=num_sampled,\n",
    "                 num_classes=vocabulary_size))\n",
    "    \n",
    "    # We use the SGD optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "    \n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  284.950469971\n",
      "Nearest to going: donut, stitches, HE, carefree, NM, ditto!, Cop, 'hi,\n",
      "Nearest to good: ino, Lunch,, knuckles, corporate, (If, :I, fantastic!!!, cant,,\n",
      "Nearest to off: Vince, lactose, concert,, Lloyd, skinned, summmer, glamour, Bonding,\n",
      "Nearest to too: WHEN, crossed., construction, retweet., hobo, reason., mere, plate.,\n",
      "Nearest to last: links., find,, clearing, Wana, pooped, Jen!, FEW, boring.,\n",
      "Nearest to am: True,, &quot;its, FINALLY., xrays, Kerry, CRAP, @nickjfrost, vocabulary,\n",
      "Nearest to this: Revolution, no!!!!, insomnia!, flames, greatly., devil, accounts,, uh,,\n",
      "Nearest to you: training., @ToddBrink, @jaykpurdy, @vagelisv, soundly, medieval, smug, Holland,\n",
      "Nearest to in: ADORABLE, reminder,, easy?, WSOP, messy, GMT, graduated!, Buddha,\n",
      "Nearest to but: Bournemouth, Aquarium, @JessicaGottlieb, yummy!, babyshower, Crossing, there), @staaceeyy,\n",
      "Nearest to it's: school,, expires, quiet,, on-line, Notts, Possible, Canon, sims3,\n",
      "Nearest to lol: TON, Dove, pm, purses, cabinets, Bump, murder, sayin',\n",
      "Nearest to my: Czech, after..., @DavidArchie, misery, WEEK!, office, mag, x.x,\n",
      "Nearest to 2: jessica, exceptional, wizards, Track, muffin., onion,, challenge!, 2night!,\n",
      "Nearest to get: ago, HEY!, tinkerbell, recommended, people's, emoticon, email., backups,\n",
      "Nearest to been: Awh,, spendin, Friends, flash., tonights, Sorry.., eBay, beach,,\n",
      "Average loss at step  2000 :  112.908735767\n",
      "Average loss at step  4000 :  53.5057980819\n",
      "Average loss at step  6000 :  33.8504986115\n",
      "Average loss at step  8000 :  23.9991013938\n",
      "Average loss at step  10000 :  17.6480908347\n",
      "Nearest to going: #seb-day, @Donnette, donut, much!, @Dr_DinaSadik, good., 'hi, banana,\n",
      "Nearest to good: @songzyuuup, great, @DoctorJohnSmith, #seb-day, @Donnette, Homemade, #frenchieb-day, n,\n",
      "Nearest to off: #frenchieb-day, ice, concert,, @dbdc, give, holys**t, Grocery, @DevineNews,\n",
      "Nearest to too: @hawkcam, #seb-day, @downesy, grade!, school, Little, ya'll, unit,\n",
      "Nearest to last: pooped, @DazzleMeThis, fricken, Potter, busy, @downesy, for, clearing,\n",
      "Nearest to am: @drunkenmonkey87, @dfizzy, doesnâ€™t, pitch, Headache., wouldn't, hahaha., and,\n",
      "Nearest to this: hundred, one., the, booked, devil, heard, that, tune,\n",
      "Nearest to you: u, I, @earthXplorer, myself, film, reminding, watery, install,\n",
      "Nearest to in: for, with, at, to, and, on, by, #seb-day,\n",
      "Nearest to but: #seb-day, , and, @DevineNews, #frenchieb-day, @DoctorJohnSmith, enter, whip,\n",
      "Nearest to it's: Grocery, Leighton, hmm.., school,, #seb-day, made, is, lift,\n",
      "Nearest to lol: UNK, , pm, Fish, gahhh, TON, @DustinJMcClure, feel,\n",
      "Nearest to my: the, #seb-day, her, chain, a, his, @dwgirl4life, @docbaty,\n",
      "Nearest to 2: jessica, to, Grrr..., bless, download?, 2night!, deffo, ha,\n",
      "Nearest to get: #frenchieb-day, #seb-day, Grocery, Hmmmm,, bought, heey, thats, ago,\n",
      "Nearest to been: Friends, @Dichenlachman, self, anything, tonights, beach,, fun!, got,\n",
      "Average loss at step  12000 :  14.220803808\n",
      "Average loss at step  14000 :  11.5526966429\n",
      "Average loss at step  16000 :  9.80241886079\n",
      "Average loss at step  18000 :  8.50396884644\n",
      "Average loss at step  20000 :  7.77927901053\n",
      "Nearest to going: #seb-day, @Donnette, donut, @emalea, 'hi, banana, good., Sunday...,\n",
      "Nearest to good: great, @songzyuuup, @DoctorJohnSmith, #seb-day, #frenchieb-day, long, @Donnette, Homemade,\n",
      "Nearest to off: #frenchieb-day, concert,, @fan4lyf, @flicka47, 4.5, ice, knows,, @EthanSuplee,\n",
      "Nearest to too: so, end.., @emalea, @downesy, @Firequacker, grade!, Little, crossed.,\n",
      "Nearest to last: fricken, @ExocetAU, pooped, best, in, @Enamoredsoul, for, #tokiohotel,\n",
      "Nearest to am: @EmilyAllTimeLow, @erin82883, was, I'm, doesnâ€™t, @dfizzy, , @drunkenmonkey87,\n",
      "Nearest to this: the, that, hundred, hush, devil, a, lol,, @Enamoredsoul,\n",
      "Nearest to you: u, I, i, @earthXplorer, they, @EmilyAllTimeLow, myself, #seb-day,\n",
      "Nearest to in: on, for, at, with, and, to, of, by,\n",
      "Nearest to but: and, @EmmaRileySutton, , #seb-day, #frenchieb-day, @DevineNews, UNK, Skinny,\n",
      "Nearest to it's: Leighton, Grocery, is, hmm.., @Emsy, school,, @Enamoredsoul, im,\n",
      "Nearest to lol: pm, , UNK, Fish, @Enamoredsoul, I, LOL, gahhh,\n",
      "Nearest to my: the, her, his, your, a, @docbaty, #seb-day, chain,\n",
      "Nearest to 2: to, @gerrymoth, jessica, Grrr..., download?, @ginoboi, @EmilyAllTimeLow, deffo,\n",
      "Nearest to get: #frenchieb-day, spend, be, #seb-day, do, bought, make, bumble,\n",
      "Nearest to been: got, @EvilNanny, @freddurst, Friends, tonights, @Dichenlachman, self, @EmilyAllTimeLow,\n",
      "Average loss at step  22000 :  7.00232000065\n",
      "Average loss at step  24000 :  6.61352243388\n",
      "Average loss at step  26000 :  6.4130946418\n",
      "Average loss at step  28000 :  6.25037951136\n",
      "Average loss at step  30000 :  5.81767050624\n",
      "Nearest to going: #seb-day, @Donnette, #bradiewebb, donut, Sunday..., 'hi, @hobosexual, @hot30,\n",
      "Nearest to good: great, @songzyuuup, @DoctorJohnSmith, bad, long, hard, #seb-day, #frenchieb-day,\n",
      "Nearest to off: #frenchieb-day, Hot!, @flicka47, @fan4lyf, concert,, all, 4.5, got,,\n",
      "Nearest to too: so, end.., @emalea, , @downesy, kiwi, @Firequacker, grade!,\n",
      "Nearest to last: fricken, best, @ExocetAU, pooped, encouraging, for, in, @Enamoredsoul,\n",
      "Nearest to am: @EmilyAllTimeLow, I'm, was, hate, @HeartMileyCyrus, @ItsNeet, @erin82883, @greekdude,\n",
      "Nearest to this: the, that, hundred, it, hush, a, devil, mo.,\n",
      "Nearest to you: u, they, i, I, @earthXplorer, @hellorachael, @EmilyAllTimeLow, @I_Support_DemiL,\n",
      "Nearest to in: on, at, with, for, by, from, and, to,\n",
      "Nearest to but: and, @EmmaRileySutton, #seb-day, @HeartMileyCyrus, #frenchieb-day, , Skinny, @hollyalyxfinch,\n",
      "Nearest to it's: its, im, Leighton, is, I'm, Grocery, hmm.., @Emsy,\n",
      "Nearest to lol: , @hellorachael, pm, @hypnophil, LOL, @I_Support_DemiL, @Enamoredsoul, UNK,\n",
      "Nearest to my: the, his, her, your, a, chain, My, @Hitman1971,\n",
      "Nearest to 2: to, @gerrymoth, question., deffo, @ginoboi, jessica, Hearts, Grrr...,\n",
      "Nearest to get: have, #frenchieb-day, @iamjersey, be, @iCharlotte, go, make, #seb-day,\n",
      "Nearest to been: got, @EvilNanny, @freddurst, Friends, two!, @gtvone, tonights, 23rd,\n",
      "Average loss at step  32000 :  5.52532606328\n",
      "Average loss at step  34000 :  5.43671243548\n",
      "Average loss at step  36000 :  5.32504051685\n",
      "Average loss at step  38000 :  5.12526505756\n",
      "Average loss at step  40000 :  5.15435396528\n",
      "Nearest to going: @jeweljk, @jamieharrington, #seb-day, having, go, @Donnette, #bradiewebb, @hot30,\n",
      "Nearest to good: great, @songzyuuup, bad, long, @DoctorJohnSmith, hard, #seb-day, #frenchieb-day,\n",
      "Nearest to off: #frenchieb-day, Hot!, @flicka47, concert,, @Jamerichin, @fan4lyf, #wnbaopeningday, got,,\n",
      "Nearest to too: so, , end.., @emalea, kiwi, @downesy, grade!, very,\n",
      "Nearest to last: fricken, best, @ExocetAU, encouraging, selective, at, pooped, O_O,\n",
      "Nearest to am: I'm, was, hate, im, @EmilyAllTimeLow, i'm, @HeartMileyCyrus, @erin82883,\n",
      "Nearest to this: the, that, hundred, it, Hot!, mo., @itschristablack, hush,\n",
      "Nearest to you: u, they, I, i, we, @earthXplorer, @EmilyAllTimeLow, #votemcfly,\n",
      "Nearest to in: on, at, with, for, from, by, to, into,\n",
      "Nearest to but: and, @EmmaRileySutton, , #seb-day, -, @HeartMileyCyrus, @JaylaStarr, &amp;,\n",
      "Nearest to it's: its, im, I'm, Leighton, is, Grocery, hmm.., @Emsy,\n",
      "Nearest to lol: , @hellorachael, LOL, pm, @hypnophil, @I_Support_DemiL, @Enamoredsoul, Fish,\n",
      "Nearest to my: your, his, the, her, a, My, chain, Hot!,\n",
      "Nearest to 2: to, @Jamerichin, @gerrymoth, deffo, question., 7, @ginoboi, @EmilyAllTimeLow,\n",
      "Nearest to get: have, be, make, go, @iamjersey, #frenchieb-day, @iCharlotte, got,\n",
      "Nearest to been: got, @EvilNanny, @freddurst, two!, EXAMS, tonights, Friends, @gtvone,\n",
      "Average loss at step  42000 :  5.04307745349\n",
      "Average loss at step  44000 :  4.83381287873\n",
      "Average loss at step  46000 :  5.05168245995\n",
      "Average loss at step  48000 :  4.98165911829\n",
      "Average loss at step  50000 :  4.84078740656\n",
      "Nearest to going: #seb-day, @jamieharrington, @jeweljk, #bradiewebb, go, @emalea, @Donnette, #votemcfly,\n",
      "Nearest to good: great, bad, long, @songzyuuup, hard, nice, @DoctorJohnSmith, #seb-day,\n",
      "Nearest to off: back, #frenchieb-day, Hot!, #wnbaopeningday, @flicka47, @Jamerichin, concert,, Il,\n",
      "Nearest to too: so, @emalea, very, end.., Hot!, @Jason_Manford, kiwi, Haha...,\n",
      "Nearest to last: fricken, best, @ExocetAU, selective, encouraging, pooped, O_O, @Enamoredsoul,\n",
      "Nearest to am: I'm, is, was, hate, im, @EmilyAllTimeLow, @HeartMileyCyrus, i'm,\n",
      "Nearest to this: the, that, hundred, it, @itschristablack, your, hush, Hot!,\n",
      "Nearest to you: u, they, we, I, i, @earthXplorer, me, @hellorachael,\n",
      "Nearest to in: on, at, with, @iamjersey, to, for, from, into,\n",
      "Nearest to but: and, @EmmaRileySutton, -, &amp;, #seb-day, @JaylaStarr, @HeartMileyCyrus, @JacobLovie,\n",
      "Nearest to it's: its, im, It's, I'm, Grocery, Leighton, i'm, @justinjap,\n",
      "Nearest to lol: , @hellorachael, LOL, @I_Support_DemiL, @hypnophil, @Enamoredsoul, pm, @juanbarnard,\n",
      "Nearest to my: her, his, your, the, My, @Hitman1971, Hot!, chain,\n",
      "Nearest to 2: to, @Jamerichin, 7, @gerrymoth, deffo, @ginoboi, 3, 4,\n",
      "Nearest to get: have, make, be, got, go, @iamjersey, #frenchieb-day, want,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to been: got, @EvilNanny, EXAMS, still, @freddurst, two!, be, Friends,\n",
      "Average loss at step  52000 :  4.77008590341\n",
      "Average loss at step  54000 :  4.87522189641\n",
      "Average loss at step  56000 :  4.6689874723\n",
      "Average loss at step  58000 :  4.82599947155\n",
      "Average loss at step  60000 :  4.95457962716\n",
      "Nearest to going: @jamieharrington, @jeweljk, #seb-day, #alltimelow, go, getting, supposed, having,\n",
      "Nearest to good: great, bad, @songzyuuup, long, #alltimelow, nice, hard, #seb-day,\n",
      "Nearest to off: back, #frenchieb-day, #wnbaopeningday, Hot!, @flicka47, @Jamerichin, concert,, @fan4lyf,\n",
      "Nearest to too: so, very, @emalea, end.., soo, Haha..., really, @Jason_Manford,\n",
      "Nearest to last: fricken, @ExocetAU, best, encouraging, selective, pooped, #tokiohotel, GUESS,\n",
      "Nearest to am: I'm, is, was, im, hate, @EmilyAllTimeLow, @HeartMileyCyrus, i'm,\n",
      "Nearest to this: the, that, hundred, it, @itschristablack, Hot!, hush, #alltimelow,\n",
      "Nearest to you: u, they, we, I, i, #alltimelow, @earthXplorer, @lemongeneration,\n",
      "Nearest to in: on, at, with, from, for, into, @iamjersey, to,\n",
      "Nearest to but: and, #USAWantsMcFly, -, &amp;, so, @EmmaRileySutton, @JaylaStarr, @lemongeneration,\n",
      "Nearest to it's: its, I'm, It's, im, i'm, Leighton, #alltimelow, @justinjap,\n",
      "Nearest to lol: , LOL, @hellorachael, @I_Support_DemiL, #alltimelow, @hypnophil, @Enamoredsoul, @Lady_Twitster,\n",
      "Nearest to my: her, his, your, the, My, a, our, @Hitman1971,\n",
      "Nearest to 2: to, @Jamerichin, 7, 3, 4, @gerrymoth, @ginoboi, 5,\n",
      "Nearest to get: got, go, have, be, make, getting, #frenchieb-day, #seb-day,\n",
      "Nearest to been: got, EXAMS, @EvilNanny, still, #alltimelow, two!, be, not,\n",
      "Average loss at step  62000 :  4.64375487518\n",
      "Average loss at step  64000 :  4.70488286793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-498-3967927aa30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# We perform one update step by evaluating the optimizer op (including it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# in the list of returned values for session.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100_001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch_skip_gram(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = np.load('./embedding_information/tweets_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the final embedding\n",
    "np.save('./embedding_information/tweets_embedding',final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average length of tweets in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in X_all:\n",
    "    avg_lengths.append(len( tweet.split(' ') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.47882633604711"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(avg_lengths) # avg tweet lenth use 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training matrix that has the vector for each word in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_matrix(tweet_matrix, word_dictionary_idx_lookup,g, embedding_size, tweet_length):\n",
    "    \"\"\"Returns a 3 dimensional tensor\n",
    "    Dim 1 (rows): the word from each tweet\n",
    "    dim 2 (columns): the vector representation of that word (128 dimensions)\n",
    "    dim 3( depth): the total number of tweets in the dataset.\n",
    "    \n",
    "    PARAMS:\n",
    "    tweet_matrix, - a matrix that contains the tweets, a list of strings\n",
    "    embedding, - the final word2vec embedding of the tweets\n",
    "    word_dictionary_idx_lookup, - the index of each word for the embedding\n",
    "    g,   - the tensorflow graph\n",
    "    embedding_size - max num. features for embedding)\n",
    "    tweet_length - how many vectors to return for each tweet\"\"\"\n",
    "    final_matrix = np.zeros((len(tweet_matrix),tweet_length))\n",
    "    for tweet_idx,tweet in enumerate(tweet_matrix):\n",
    "        tweet_idxs = []\n",
    "        for word in tweet.split(' '): # go through each word\n",
    "            if len(tweet_idxs)>=tweet_length: \n",
    "                break\n",
    "            elif word =='':\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    tweet_idxs.append(word_dictionary_idx_lookup[word])\n",
    "                except: # unknown word\n",
    "                    tweet_idxs.append(0) # vector for UNK words, unknown\n",
    "        while len(tweet_idxs)<tweet_length:\n",
    "            tweet_idxs.append(1) ## fill in with blanks\n",
    "            \n",
    "        # now, find the vectors for each word\n",
    "#         with tf.Session(graph=g) as sess:\n",
    "#             tweet_embedding = tf.nn.embedding_lookup(embedding, tweet_idxs).eval()\n",
    "        #final_matrix[:,:,tweet_idx] = tweet_embedding\n",
    "    \n",
    "        final_matrix[tweet_idx,:] = tweet_idxs\n",
    "        if tweet_idx % 100_000 ==0:\n",
    "            print(f\"Finished number {tweet_idx+1} out of {len(tweet_matrix)}\")\n",
    "    return final_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open(\"./embedding_information/dictionary_lookup_ids\",'wb') as fp:\n",
    "    pickle.dump(dictionary,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished number 1 out of 1262889\n",
      "Finished number 100001 out of 1262889\n",
      "Finished number 200001 out of 1262889\n",
      "Finished number 300001 out of 1262889\n",
      "Finished number 400001 out of 1262889\n",
      "Finished number 500001 out of 1262889\n",
      "Finished number 600001 out of 1262889\n",
      "Finished number 700001 out of 1262889\n",
      "Finished number 800001 out of 1262889\n",
      "Finished number 900001 out of 1262889\n",
      "Finished number 1000001 out of 1262889\n",
      "Finished number 1100001 out of 1262889\n",
      "Finished number 1200001 out of 1262889\n"
     ]
    }
   ],
   "source": [
    "tweet_idxs_matrix_train = create_training_matrix(X_train_lstm,  dictionary, graph ,embedding_size, max_tweet_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished number 1 out of 315723\n",
      "Finished number 100001 out of 315723\n",
      "Finished number 200001 out of 315723\n",
      "Finished number 300001 out of 315723\n"
     ]
    }
   ],
   "source": [
    "tweet_idxs_matrix_test = create_training_matrix(X_test_lstm,  dictionary, graph ,embedding_size, max_tweet_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the LSTM graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "lstmUnits = 256\n",
    "numClasses = 2\n",
    "numDimensions = 128 # size of embedding\n",
    "iterations = 150_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 out of 150000\n",
      "Iteration 50 out of 150000\n",
      "Iteration 100 out of 150000\n",
      "Iteration 150 out of 150000\n",
      "Iteration 200 out of 150000\n",
      "Iteration 250 out of 150000\n",
      "Iteration 300 out of 150000\n",
      "Iteration 350 out of 150000\n",
      "Iteration 400 out of 150000\n",
      "Iteration 450 out of 150000\n",
      "Iteration 500 out of 150000\n",
      "Iteration 550 out of 150000\n",
      "Iteration 600 out of 150000\n",
      "Iteration 650 out of 150000\n",
      "Iteration 700 out of 150000\n",
      "Iteration 750 out of 150000\n",
      "Iteration 800 out of 150000\n",
      "Iteration 850 out of 150000\n",
      "Iteration 900 out of 150000\n",
      "Iteration 950 out of 150000\n",
      "Iteration 1000 out of 150000\n",
      "Iteration 1050 out of 150000\n",
      "Iteration 1100 out of 150000\n",
      "Iteration 1150 out of 150000\n",
      "Iteration 1200 out of 150000\n",
      "Iteration 1250 out of 150000\n",
      "Iteration 1300 out of 150000\n",
      "Iteration 1350 out of 150000\n",
      "Iteration 1400 out of 150000\n",
      "Iteration 1450 out of 150000\n",
      "Iteration 1500 out of 150000\n",
      "Iteration 1550 out of 150000\n",
      "Iteration 1600 out of 150000\n",
      "Iteration 1650 out of 150000\n",
      "Iteration 1700 out of 150000\n",
      "Iteration 1750 out of 150000\n",
      "Iteration 1800 out of 150000\n",
      "Iteration 1850 out of 150000\n",
      "Iteration 1900 out of 150000\n",
      "Iteration 1950 out of 150000\n",
      "Iteration 2000 out of 150000\n",
      "Iteration 2050 out of 150000\n",
      "Iteration 2100 out of 150000\n",
      "Iteration 2150 out of 150000\n",
      "Iteration 2200 out of 150000\n",
      "Iteration 2250 out of 150000\n",
      "Iteration 2300 out of 150000\n",
      "Iteration 2350 out of 150000\n",
      "Iteration 2400 out of 150000\n",
      "Iteration 2450 out of 150000\n",
      "Iteration 2500 out of 150000\n",
      "Iteration 2550 out of 150000\n",
      "Iteration 2600 out of 150000\n",
      "Iteration 2650 out of 150000\n",
      "Iteration 2700 out of 150000\n",
      "Iteration 2750 out of 150000\n",
      "Iteration 2800 out of 150000\n",
      "Iteration 2850 out of 150000\n",
      "Iteration 2900 out of 150000\n",
      "Iteration 2950 out of 150000\n",
      "Iteration 3000 out of 150000\n",
      "Iteration 3050 out of 150000\n",
      "Iteration 3100 out of 150000\n",
      "Iteration 3150 out of 150000\n",
      "Iteration 3200 out of 150000\n",
      "Iteration 3250 out of 150000\n",
      "Iteration 3300 out of 150000\n",
      "Iteration 3350 out of 150000\n",
      "Iteration 3400 out of 150000\n",
      "Iteration 3450 out of 150000\n",
      "Iteration 3500 out of 150000\n",
      "Iteration 3550 out of 150000\n",
      "Iteration 3600 out of 150000\n",
      "Iteration 3650 out of 150000\n",
      "Iteration 3700 out of 150000\n",
      "Iteration 3750 out of 150000\n",
      "Iteration 3800 out of 150000\n",
      "Iteration 3850 out of 150000\n",
      "Iteration 3900 out of 150000\n",
      "Iteration 3950 out of 150000\n",
      "Iteration 4000 out of 150000\n",
      "Iteration 4050 out of 150000\n",
      "Iteration 4100 out of 150000\n",
      "Iteration 4150 out of 150000\n",
      "Iteration 4200 out of 150000\n",
      "Iteration 4250 out of 150000\n",
      "Iteration 4300 out of 150000\n",
      "Iteration 4350 out of 150000\n",
      "Iteration 4400 out of 150000\n",
      "Iteration 4450 out of 150000\n",
      "Iteration 4500 out of 150000\n",
      "Iteration 4550 out of 150000\n",
      "Iteration 4600 out of 150000\n",
      "Iteration 4650 out of 150000\n",
      "Iteration 4700 out of 150000\n",
      "Iteration 4750 out of 150000\n",
      "Iteration 4800 out of 150000\n",
      "Iteration 4850 out of 150000\n",
      "Iteration 4900 out of 150000\n",
      "Iteration 4950 out of 150000\n",
      "Iteration 5000 out of 150000\n",
      "Iteration 5050 out of 150000\n",
      "Iteration 5100 out of 150000\n",
      "Iteration 5150 out of 150000\n",
      "Iteration 5200 out of 150000\n",
      "Iteration 5250 out of 150000\n",
      "Iteration 5300 out of 150000\n",
      "Iteration 5350 out of 150000\n",
      "Iteration 5400 out of 150000\n",
      "Iteration 5450 out of 150000\n",
      "Iteration 5500 out of 150000\n",
      "Iteration 5550 out of 150000\n",
      "Iteration 5600 out of 150000\n",
      "Iteration 5650 out of 150000\n",
      "Iteration 5700 out of 150000\n",
      "Iteration 5750 out of 150000\n",
      "Iteration 5800 out of 150000\n",
      "Iteration 5850 out of 150000\n",
      "Iteration 5900 out of 150000\n",
      "Iteration 5950 out of 150000\n",
      "Iteration 6000 out of 150000\n",
      "Iteration 6050 out of 150000\n",
      "Iteration 6100 out of 150000\n",
      "Iteration 6150 out of 150000\n",
      "Iteration 6200 out of 150000\n",
      "Iteration 6250 out of 150000\n",
      "Iteration 6300 out of 150000\n",
      "Iteration 6350 out of 150000\n",
      "Iteration 6400 out of 150000\n",
      "Iteration 6450 out of 150000\n",
      "Iteration 6500 out of 150000\n",
      "Iteration 6550 out of 150000\n",
      "Iteration 6600 out of 150000\n",
      "Iteration 6650 out of 150000\n",
      "Iteration 6700 out of 150000\n",
      "Iteration 6750 out of 150000\n",
      "Iteration 6800 out of 150000\n",
      "Iteration 6850 out of 150000\n",
      "Iteration 6900 out of 150000\n",
      "Iteration 6950 out of 150000\n",
      "Iteration 7000 out of 150000\n",
      "Iteration 7050 out of 150000\n",
      "Iteration 7100 out of 150000\n",
      "Iteration 7150 out of 150000\n",
      "Iteration 7200 out of 150000\n",
      "Iteration 7250 out of 150000\n",
      "Iteration 7300 out of 150000\n",
      "Iteration 7350 out of 150000\n",
      "Iteration 7400 out of 150000\n",
      "Iteration 7450 out of 150000\n",
      "Iteration 7500 out of 150000\n",
      "Iteration 7550 out of 150000\n",
      "Iteration 7600 out of 150000\n",
      "Iteration 7650 out of 150000\n",
      "Iteration 7700 out of 150000\n",
      "Iteration 7750 out of 150000\n",
      "Iteration 7800 out of 150000\n",
      "Iteration 7850 out of 150000\n",
      "Iteration 7900 out of 150000\n",
      "Iteration 7950 out of 150000\n",
      "Iteration 8000 out of 150000\n",
      "Iteration 8050 out of 150000\n",
      "Iteration 8100 out of 150000\n",
      "Iteration 8150 out of 150000\n",
      "Iteration 8200 out of 150000\n",
      "Iteration 8250 out of 150000\n",
      "Iteration 8300 out of 150000\n",
      "Iteration 8350 out of 150000\n",
      "Iteration 8400 out of 150000\n",
      "Iteration 8450 out of 150000\n",
      "Iteration 8500 out of 150000\n",
      "Iteration 8550 out of 150000\n",
      "Iteration 8600 out of 150000\n",
      "Iteration 8650 out of 150000\n",
      "Iteration 8700 out of 150000\n",
      "Iteration 8750 out of 150000\n",
      "Iteration 8800 out of 150000\n",
      "Iteration 8850 out of 150000\n",
      "Iteration 8900 out of 150000\n",
      "Iteration 8950 out of 150000\n",
      "Iteration 9000 out of 150000\n",
      "Iteration 9050 out of 150000\n",
      "Iteration 9100 out of 150000\n",
      "Iteration 9150 out of 150000\n",
      "Iteration 9200 out of 150000\n",
      "Iteration 9250 out of 150000\n",
      "Iteration 9300 out of 150000\n",
      "Iteration 9350 out of 150000\n",
      "Iteration 9400 out of 150000\n",
      "Iteration 9450 out of 150000\n",
      "Iteration 9500 out of 150000\n",
      "Iteration 9550 out of 150000\n",
      "Iteration 9600 out of 150000\n",
      "Iteration 9650 out of 150000\n",
      "Iteration 9700 out of 150000\n",
      "Iteration 9750 out of 150000\n",
      "Iteration 9800 out of 150000\n",
      "Iteration 9850 out of 150000\n",
      "Iteration 9900 out of 150000\n",
      "Iteration 9950 out of 150000\n",
      "Iteration 10000 out of 150000\n",
      "saved to tensorflow/lstm_model-10000\n",
      "Iteration 10050 out of 150000\n",
      "Iteration 10100 out of 150000\n",
      "Iteration 10150 out of 150000\n",
      "Iteration 10200 out of 150000\n",
      "Iteration 10250 out of 150000\n",
      "Iteration 10300 out of 150000\n",
      "Iteration 10350 out of 150000\n",
      "Iteration 10400 out of 150000\n",
      "Iteration 10450 out of 150000\n",
      "Iteration 10500 out of 150000\n",
      "Iteration 10550 out of 150000\n",
      "Iteration 10600 out of 150000\n",
      "Iteration 10650 out of 150000\n",
      "Iteration 10700 out of 150000\n",
      "Iteration 10750 out of 150000\n",
      "Iteration 10800 out of 150000\n",
      "Iteration 10850 out of 150000\n",
      "Iteration 10900 out of 150000\n",
      "Iteration 10950 out of 150000\n",
      "Iteration 11000 out of 150000\n",
      "Iteration 11050 out of 150000\n",
      "Iteration 11100 out of 150000\n",
      "Iteration 11150 out of 150000\n",
      "Iteration 11200 out of 150000\n",
      "Iteration 11250 out of 150000\n",
      "Iteration 11300 out of 150000\n",
      "Iteration 11350 out of 150000\n",
      "Iteration 11400 out of 150000\n",
      "Iteration 11450 out of 150000\n",
      "Iteration 11500 out of 150000\n",
      "Iteration 11550 out of 150000\n",
      "Iteration 11600 out of 150000\n",
      "Iteration 11650 out of 150000\n",
      "Iteration 11700 out of 150000\n",
      "Iteration 11750 out of 150000\n",
      "Iteration 11800 out of 150000\n",
      "Iteration 11850 out of 150000\n",
      "Iteration 11900 out of 150000\n",
      "Iteration 11950 out of 150000\n",
      "Iteration 12000 out of 150000\n",
      "Iteration 12050 out of 150000\n",
      "Iteration 12100 out of 150000\n",
      "Iteration 12150 out of 150000\n",
      "Iteration 12200 out of 150000\n",
      "Iteration 12250 out of 150000\n",
      "Iteration 12300 out of 150000\n",
      "Iteration 12350 out of 150000\n",
      "Iteration 12400 out of 150000\n",
      "Iteration 12450 out of 150000\n",
      "Iteration 12500 out of 150000\n",
      "Iteration 12550 out of 150000\n",
      "Iteration 12600 out of 150000\n",
      "Iteration 12650 out of 150000\n",
      "Iteration 12700 out of 150000\n",
      "Iteration 12750 out of 150000\n",
      "Iteration 12800 out of 150000\n",
      "Iteration 12850 out of 150000\n",
      "Iteration 12900 out of 150000\n",
      "Iteration 12950 out of 150000\n",
      "Iteration 13000 out of 150000\n",
      "Iteration 13050 out of 150000\n",
      "Iteration 13100 out of 150000\n",
      "Iteration 13150 out of 150000\n",
      "Iteration 13200 out of 150000\n",
      "Iteration 13250 out of 150000\n",
      "Iteration 13300 out of 150000\n",
      "Iteration 13350 out of 150000\n",
      "Iteration 13400 out of 150000\n",
      "Iteration 13450 out of 150000\n",
      "Iteration 13500 out of 150000\n",
      "Iteration 13550 out of 150000\n",
      "Iteration 13600 out of 150000\n",
      "Iteration 13650 out of 150000\n",
      "Iteration 13700 out of 150000\n",
      "Iteration 13750 out of 150000\n",
      "Iteration 13800 out of 150000\n",
      "Iteration 13850 out of 150000\n",
      "Iteration 13900 out of 150000\n",
      "Iteration 13950 out of 150000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14000 out of 150000\n",
      "Iteration 14050 out of 150000\n",
      "Iteration 14100 out of 150000\n",
      "Iteration 14150 out of 150000\n",
      "Iteration 14200 out of 150000\n",
      "Iteration 14250 out of 150000\n",
      "Iteration 14300 out of 150000\n",
      "Iteration 14350 out of 150000\n",
      "Iteration 14400 out of 150000\n",
      "Iteration 14450 out of 150000\n",
      "Iteration 14500 out of 150000\n",
      "Iteration 14550 out of 150000\n",
      "Iteration 14600 out of 150000\n",
      "Iteration 14650 out of 150000\n",
      "Iteration 14700 out of 150000\n",
      "Iteration 14750 out of 150000\n",
      "Iteration 14800 out of 150000\n",
      "Iteration 14850 out of 150000\n",
      "Iteration 14900 out of 150000\n",
      "Iteration 14950 out of 150000\n",
      "Iteration 15000 out of 150000\n",
      "Iteration 15050 out of 150000\n",
      "Iteration 15100 out of 150000\n",
      "Iteration 15150 out of 150000\n",
      "Iteration 15200 out of 150000\n",
      "Iteration 15250 out of 150000\n",
      "Iteration 15300 out of 150000\n",
      "Iteration 15350 out of 150000\n",
      "Iteration 15400 out of 150000\n",
      "Iteration 15450 out of 150000\n",
      "Iteration 15500 out of 150000\n",
      "Iteration 15550 out of 150000\n",
      "Iteration 15600 out of 150000\n",
      "Iteration 15650 out of 150000\n",
      "Iteration 15700 out of 150000\n",
      "Iteration 15750 out of 150000\n",
      "Iteration 15800 out of 150000\n",
      "Iteration 15850 out of 150000\n",
      "Iteration 15900 out of 150000\n",
      "Iteration 15950 out of 150000\n",
      "Iteration 16000 out of 150000\n",
      "Iteration 16050 out of 150000\n",
      "Iteration 16100 out of 150000\n",
      "Iteration 16150 out of 150000\n",
      "Iteration 16200 out of 150000\n",
      "Iteration 16250 out of 150000\n",
      "Iteration 16300 out of 150000\n",
      "Iteration 16350 out of 150000\n",
      "Iteration 16400 out of 150000\n",
      "Iteration 16450 out of 150000\n",
      "Iteration 16500 out of 150000\n",
      "Iteration 16550 out of 150000\n",
      "Iteration 16600 out of 150000\n",
      "Iteration 16650 out of 150000\n",
      "Iteration 16700 out of 150000\n",
      "Iteration 16750 out of 150000\n",
      "Iteration 16800 out of 150000\n",
      "Iteration 16850 out of 150000\n",
      "Iteration 16900 out of 150000\n",
      "Iteration 16950 out of 150000\n",
      "Iteration 17000 out of 150000\n",
      "Iteration 17050 out of 150000\n",
      "Iteration 17100 out of 150000\n",
      "Iteration 17150 out of 150000\n",
      "Iteration 17200 out of 150000\n",
      "Iteration 17250 out of 150000\n",
      "Iteration 17300 out of 150000\n",
      "Iteration 17350 out of 150000\n",
      "Iteration 17400 out of 150000\n",
      "Iteration 17450 out of 150000\n",
      "Iteration 17500 out of 150000\n",
      "Iteration 17550 out of 150000\n",
      "Iteration 17600 out of 150000\n",
      "Iteration 17650 out of 150000\n",
      "Iteration 17700 out of 150000\n",
      "Iteration 17750 out of 150000\n",
      "Iteration 17800 out of 150000\n",
      "Iteration 17850 out of 150000\n",
      "Iteration 17900 out of 150000\n",
      "Iteration 17950 out of 150000\n",
      "Iteration 18000 out of 150000\n",
      "Iteration 18050 out of 150000\n",
      "Iteration 18100 out of 150000\n",
      "Iteration 18150 out of 150000\n",
      "Iteration 18200 out of 150000\n",
      "Iteration 18250 out of 150000\n",
      "Iteration 18300 out of 150000\n",
      "Iteration 18350 out of 150000\n",
      "Iteration 18400 out of 150000\n",
      "Iteration 18450 out of 150000\n",
      "Iteration 18500 out of 150000\n",
      "Iteration 18550 out of 150000\n",
      "Iteration 18600 out of 150000\n",
      "Iteration 18650 out of 150000\n",
      "Iteration 18700 out of 150000\n",
      "Iteration 18750 out of 150000\n",
      "Iteration 18800 out of 150000\n",
      "Iteration 18850 out of 150000\n",
      "Iteration 18900 out of 150000\n",
      "Iteration 18950 out of 150000\n",
      "Iteration 19000 out of 150000\n",
      "Iteration 19050 out of 150000\n",
      "Iteration 19100 out of 150000\n",
      "Iteration 19150 out of 150000\n",
      "Iteration 19200 out of 150000\n",
      "Iteration 19250 out of 150000\n",
      "Iteration 19300 out of 150000\n",
      "Iteration 19350 out of 150000\n",
      "Iteration 19400 out of 150000\n",
      "Iteration 19450 out of 150000\n",
      "Iteration 19500 out of 150000\n",
      "Iteration 19550 out of 150000\n",
      "Iteration 19600 out of 150000\n",
      "Iteration 19650 out of 150000\n",
      "Iteration 19700 out of 150000\n",
      "Iteration 19750 out of 150000\n",
      "Iteration 19800 out of 150000\n",
      "Iteration 19850 out of 150000\n",
      "Iteration 19900 out of 150000\n",
      "Iteration 19950 out of 150000\n",
      "Iteration 20000 out of 150000\n",
      "saved to tensorflow/lstm_model-20000\n",
      "Iteration 20050 out of 150000\n",
      "Iteration 20100 out of 150000\n",
      "Iteration 20150 out of 150000\n",
      "Iteration 20200 out of 150000\n",
      "Iteration 20250 out of 150000\n",
      "Iteration 20300 out of 150000\n",
      "Iteration 20350 out of 150000\n",
      "Iteration 20400 out of 150000\n",
      "Iteration 20450 out of 150000\n",
      "Iteration 20500 out of 150000\n",
      "Iteration 20550 out of 150000\n",
      "Iteration 20600 out of 150000\n",
      "Iteration 20650 out of 150000\n",
      "Iteration 20700 out of 150000\n",
      "Iteration 20750 out of 150000\n",
      "Iteration 20800 out of 150000\n",
      "Iteration 20850 out of 150000\n",
      "Iteration 20900 out of 150000\n",
      "Iteration 20950 out of 150000\n",
      "Iteration 21000 out of 150000\n",
      "Iteration 21050 out of 150000\n",
      "Iteration 21100 out of 150000\n",
      "Iteration 21150 out of 150000\n",
      "Iteration 21200 out of 150000\n",
      "Iteration 21250 out of 150000\n",
      "Iteration 21300 out of 150000\n",
      "Iteration 21350 out of 150000\n",
      "Iteration 21400 out of 150000\n",
      "Iteration 21450 out of 150000\n",
      "Iteration 21500 out of 150000\n",
      "Iteration 21550 out of 150000\n",
      "Iteration 21600 out of 150000\n",
      "Iteration 21650 out of 150000\n",
      "Iteration 21700 out of 150000\n",
      "Iteration 21750 out of 150000\n",
      "Iteration 21800 out of 150000\n",
      "Iteration 21850 out of 150000\n",
      "Iteration 21900 out of 150000\n",
      "Iteration 21950 out of 150000\n",
      "Iteration 22000 out of 150000\n",
      "Iteration 22050 out of 150000\n",
      "Iteration 22100 out of 150000\n",
      "Iteration 22150 out of 150000\n",
      "Iteration 22200 out of 150000\n",
      "Iteration 22250 out of 150000\n",
      "Iteration 22300 out of 150000\n",
      "Iteration 22350 out of 150000\n",
      "Iteration 22400 out of 150000\n",
      "Iteration 22450 out of 150000\n",
      "Iteration 22500 out of 150000\n",
      "Iteration 22550 out of 150000\n",
      "Iteration 22600 out of 150000\n",
      "Iteration 22650 out of 150000\n",
      "Iteration 22700 out of 150000\n",
      "Iteration 22750 out of 150000\n",
      "Iteration 22800 out of 150000\n",
      "Iteration 22850 out of 150000\n",
      "Iteration 22900 out of 150000\n",
      "Iteration 22950 out of 150000\n",
      "Iteration 23000 out of 150000\n",
      "Iteration 23050 out of 150000\n",
      "Iteration 23100 out of 150000\n",
      "Iteration 23150 out of 150000\n",
      "Iteration 23200 out of 150000\n",
      "Iteration 23250 out of 150000\n",
      "Iteration 23300 out of 150000\n",
      "Iteration 23350 out of 150000\n",
      "Iteration 23400 out of 150000\n",
      "Iteration 23450 out of 150000\n",
      "Iteration 23500 out of 150000\n",
      "Iteration 23550 out of 150000\n",
      "Iteration 23600 out of 150000\n",
      "Iteration 23650 out of 150000\n",
      "Iteration 23700 out of 150000\n",
      "Iteration 23750 out of 150000\n",
      "Iteration 23800 out of 150000\n",
      "Iteration 23850 out of 150000\n",
      "Iteration 23900 out of 150000\n",
      "Iteration 23950 out of 150000\n",
      "Iteration 24000 out of 150000\n",
      "Iteration 24050 out of 150000\n",
      "Iteration 24100 out of 150000\n",
      "Iteration 24150 out of 150000\n",
      "Iteration 24200 out of 150000\n",
      "Iteration 24250 out of 150000\n",
      "Iteration 24300 out of 150000\n",
      "Iteration 24350 out of 150000\n",
      "Iteration 24400 out of 150000\n",
      "Iteration 24450 out of 150000\n",
      "Iteration 24500 out of 150000\n",
      "Iteration 24550 out of 150000\n",
      "Iteration 24600 out of 150000\n",
      "Iteration 24650 out of 150000\n",
      "Iteration 24700 out of 150000\n",
      "Iteration 24750 out of 150000\n",
      "Iteration 24800 out of 150000\n",
      "Iteration 24850 out of 150000\n",
      "Iteration 24900 out of 150000\n",
      "Iteration 24950 out of 150000\n",
      "Iteration 25000 out of 150000\n",
      "Iteration 25050 out of 150000\n",
      "Iteration 25100 out of 150000\n",
      "Iteration 25150 out of 150000\n",
      "Iteration 25200 out of 150000\n",
      "Iteration 25250 out of 150000\n",
      "Iteration 25300 out of 150000\n",
      "Iteration 25350 out of 150000\n",
      "Iteration 25400 out of 150000\n",
      "Iteration 25450 out of 150000\n",
      "Iteration 25500 out of 150000\n",
      "Iteration 25550 out of 150000\n",
      "Iteration 25600 out of 150000\n",
      "Iteration 25650 out of 150000\n",
      "Iteration 25700 out of 150000\n",
      "Iteration 25750 out of 150000\n",
      "Iteration 25800 out of 150000\n",
      "Iteration 25850 out of 150000\n",
      "Iteration 25900 out of 150000\n",
      "Iteration 25950 out of 150000\n",
      "Iteration 26000 out of 150000\n",
      "Iteration 26050 out of 150000\n",
      "Iteration 26100 out of 150000\n",
      "Iteration 26150 out of 150000\n",
      "Iteration 26200 out of 150000\n",
      "Iteration 26250 out of 150000\n",
      "Iteration 26300 out of 150000\n",
      "Iteration 26350 out of 150000\n",
      "Iteration 26400 out of 150000\n",
      "Iteration 26450 out of 150000\n",
      "Iteration 26500 out of 150000\n",
      "Iteration 26550 out of 150000\n",
      "Iteration 26600 out of 150000\n",
      "Iteration 26650 out of 150000\n",
      "Iteration 26700 out of 150000\n",
      "Iteration 26750 out of 150000\n",
      "Iteration 26800 out of 150000\n",
      "Iteration 26850 out of 150000\n",
      "Iteration 26900 out of 150000\n",
      "Iteration 26950 out of 150000\n",
      "Iteration 27000 out of 150000\n",
      "Iteration 27050 out of 150000\n",
      "Iteration 27100 out of 150000\n",
      "Iteration 27150 out of 150000\n",
      "Iteration 27200 out of 150000\n",
      "Iteration 27250 out of 150000\n",
      "Iteration 27300 out of 150000\n",
      "Iteration 27350 out of 150000\n",
      "Iteration 27400 out of 150000\n",
      "Iteration 27450 out of 150000\n",
      "Iteration 27500 out of 150000\n",
      "Iteration 27550 out of 150000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27600 out of 150000\n",
      "Iteration 27650 out of 150000\n",
      "Iteration 27700 out of 150000\n",
      "Iteration 27750 out of 150000\n",
      "Iteration 27800 out of 150000\n",
      "Iteration 27850 out of 150000\n",
      "Iteration 27900 out of 150000\n",
      "Iteration 27950 out of 150000\n",
      "Iteration 28000 out of 150000\n",
      "Iteration 28050 out of 150000\n",
      "Iteration 28100 out of 150000\n",
      "Iteration 28150 out of 150000\n",
      "Iteration 28200 out of 150000\n",
      "Iteration 28250 out of 150000\n",
      "Iteration 28300 out of 150000\n",
      "Iteration 28350 out of 150000\n",
      "Iteration 28400 out of 150000\n",
      "Iteration 28450 out of 150000\n",
      "Iteration 28500 out of 150000\n",
      "Iteration 28550 out of 150000\n",
      "Iteration 28600 out of 150000\n",
      "Iteration 28650 out of 150000\n",
      "Iteration 28700 out of 150000\n",
      "Iteration 28750 out of 150000\n",
      "Iteration 28800 out of 150000\n",
      "Iteration 28850 out of 150000\n",
      "Iteration 28900 out of 150000\n",
      "Iteration 28950 out of 150000\n",
      "Iteration 29000 out of 150000\n",
      "Iteration 29050 out of 150000\n",
      "Iteration 29100 out of 150000\n",
      "Iteration 29150 out of 150000\n",
      "Iteration 29200 out of 150000\n",
      "Iteration 29250 out of 150000\n",
      "Iteration 29300 out of 150000\n",
      "Iteration 29350 out of 150000\n",
      "Iteration 29400 out of 150000\n",
      "Iteration 29450 out of 150000\n",
      "Iteration 29500 out of 150000\n",
      "Iteration 29550 out of 150000\n",
      "Iteration 29600 out of 150000\n",
      "Iteration 29650 out of 150000\n",
      "Iteration 29700 out of 150000\n",
      "Iteration 29750 out of 150000\n",
      "Iteration 29800 out of 150000\n",
      "Iteration 29850 out of 150000\n",
      "Iteration 29900 out of 150000\n",
      "Iteration 29950 out of 150000\n",
      "Iteration 30000 out of 150000\n",
      "saved to tensorflow/lstm_model-30000\n",
      "Iteration 30050 out of 150000\n",
      "Iteration 30100 out of 150000\n",
      "Iteration 30150 out of 150000\n",
      "Iteration 30200 out of 150000\n",
      "Iteration 30250 out of 150000\n",
      "Iteration 30300 out of 150000\n",
      "Iteration 30350 out of 150000\n",
      "Iteration 30400 out of 150000\n",
      "Iteration 30450 out of 150000\n",
      "Iteration 30500 out of 150000\n",
      "Iteration 30550 out of 150000\n",
      "Iteration 30600 out of 150000\n",
      "Iteration 30650 out of 150000\n",
      "Iteration 30700 out of 150000\n",
      "Iteration 30750 out of 150000\n",
      "Iteration 30800 out of 150000\n",
      "Iteration 30850 out of 150000\n",
      "Iteration 30900 out of 150000\n",
      "Iteration 30950 out of 150000\n",
      "Iteration 31000 out of 150000\n",
      "Iteration 31050 out of 150000\n",
      "Iteration 31100 out of 150000\n",
      "Iteration 31150 out of 150000\n",
      "Iteration 31200 out of 150000\n",
      "Iteration 31250 out of 150000\n",
      "Iteration 31300 out of 150000\n",
      "Iteration 31350 out of 150000\n",
      "Iteration 31400 out of 150000\n",
      "Iteration 31450 out of 150000\n",
      "Iteration 31500 out of 150000\n",
      "Iteration 31550 out of 150000\n",
      "Iteration 31600 out of 150000\n",
      "Iteration 31650 out of 150000\n",
      "Iteration 31700 out of 150000\n",
      "Iteration 31750 out of 150000\n",
      "Iteration 31800 out of 150000\n",
      "Iteration 31850 out of 150000\n",
      "Iteration 31900 out of 150000\n",
      "Iteration 31950 out of 150000\n",
      "Iteration 32000 out of 150000\n",
      "Iteration 32050 out of 150000\n",
      "Iteration 32100 out of 150000\n",
      "Iteration 32150 out of 150000\n",
      "Iteration 32200 out of 150000\n",
      "Iteration 32250 out of 150000\n",
      "Iteration 32300 out of 150000\n",
      "Iteration 32350 out of 150000\n",
      "Iteration 32400 out of 150000\n",
      "Iteration 32450 out of 150000\n",
      "Iteration 32500 out of 150000\n",
      "Iteration 32550 out of 150000\n",
      "Iteration 32600 out of 150000\n",
      "Iteration 32650 out of 150000\n",
      "Iteration 32700 out of 150000\n",
      "Iteration 32750 out of 150000\n",
      "Iteration 32800 out of 150000\n",
      "Iteration 32850 out of 150000\n",
      "Iteration 32900 out of 150000\n",
      "Iteration 32950 out of 150000\n",
      "Iteration 33000 out of 150000\n",
      "Iteration 33050 out of 150000\n",
      "Iteration 33100 out of 150000\n",
      "Iteration 33150 out of 150000\n",
      "Iteration 33200 out of 150000\n",
      "Iteration 33250 out of 150000\n",
      "Iteration 33300 out of 150000\n",
      "Iteration 33350 out of 150000\n",
      "Iteration 33400 out of 150000\n",
      "Iteration 33450 out of 150000\n",
      "Iteration 33500 out of 150000\n",
      "Iteration 33550 out of 150000\n",
      "Iteration 33600 out of 150000\n",
      "Iteration 33650 out of 150000\n",
      "Iteration 33700 out of 150000\n",
      "Iteration 33750 out of 150000\n",
      "Iteration 33800 out of 150000\n",
      "Iteration 33850 out of 150000\n",
      "Iteration 33900 out of 150000\n",
      "Iteration 33950 out of 150000\n",
      "Iteration 34000 out of 150000\n",
      "Iteration 34050 out of 150000\n",
      "Iteration 34100 out of 150000\n",
      "Iteration 34150 out of 150000\n",
      "Iteration 34200 out of 150000\n",
      "Iteration 34250 out of 150000\n",
      "Iteration 34300 out of 150000\n",
      "Iteration 34350 out of 150000\n",
      "Iteration 34400 out of 150000\n",
      "Iteration 34450 out of 150000\n",
      "Iteration 34500 out of 150000\n",
      "Iteration 34550 out of 150000\n",
      "Iteration 34600 out of 150000\n",
      "Iteration 34650 out of 150000\n",
      "Iteration 34700 out of 150000\n",
      "Iteration 34750 out of 150000\n",
      "Iteration 34800 out of 150000\n",
      "Iteration 34850 out of 150000\n",
      "Iteration 34900 out of 150000\n",
      "Iteration 34950 out of 150000\n",
      "Iteration 35000 out of 150000\n",
      "Iteration 35050 out of 150000\n",
      "Iteration 35100 out of 150000\n",
      "Iteration 35150 out of 150000\n",
      "Iteration 35200 out of 150000\n",
      "Iteration 35250 out of 150000\n",
      "Iteration 35300 out of 150000\n",
      "Iteration 35350 out of 150000\n",
      "Iteration 35400 out of 150000\n",
      "Iteration 35450 out of 150000\n",
      "Iteration 35500 out of 150000\n",
      "Iteration 35550 out of 150000\n",
      "Iteration 35600 out of 150000\n",
      "Iteration 35650 out of 150000\n",
      "Iteration 35700 out of 150000\n",
      "Iteration 35750 out of 150000\n",
      "Iteration 35800 out of 150000\n",
      "Iteration 35850 out of 150000\n",
      "Iteration 35900 out of 150000\n",
      "Iteration 35950 out of 150000\n",
      "Iteration 36000 out of 150000\n",
      "Iteration 36050 out of 150000\n",
      "Iteration 36100 out of 150000\n",
      "Iteration 36150 out of 150000\n",
      "Iteration 36200 out of 150000\n",
      "Iteration 36250 out of 150000\n",
      "Iteration 36300 out of 150000\n",
      "Iteration 36350 out of 150000\n",
      "Iteration 36400 out of 150000\n",
      "Iteration 36450 out of 150000\n",
      "Iteration 36500 out of 150000\n",
      "Iteration 36550 out of 150000\n",
      "Iteration 36600 out of 150000\n",
      "Iteration 36650 out of 150000\n",
      "Iteration 36700 out of 150000\n",
      "Iteration 36750 out of 150000\n",
      "Iteration 36800 out of 150000\n",
      "Iteration 36850 out of 150000\n",
      "Iteration 36900 out of 150000\n",
      "Iteration 36950 out of 150000\n",
      "Iteration 37000 out of 150000\n",
      "Iteration 37050 out of 150000\n",
      "Iteration 37100 out of 150000\n",
      "Iteration 37150 out of 150000\n",
      "Iteration 37200 out of 150000\n",
      "Iteration 37250 out of 150000\n",
      "Iteration 37300 out of 150000\n",
      "Iteration 37350 out of 150000\n",
      "Iteration 37400 out of 150000\n",
      "Iteration 37450 out of 150000\n",
      "Iteration 37500 out of 150000\n",
      "Iteration 37550 out of 150000\n",
      "Iteration 37600 out of 150000\n",
      "Iteration 37650 out of 150000\n",
      "Iteration 37700 out of 150000\n",
      "Iteration 37750 out of 150000\n",
      "Iteration 37800 out of 150000\n",
      "Iteration 37850 out of 150000\n",
      "Iteration 37900 out of 150000\n",
      "Iteration 37950 out of 150000\n",
      "Iteration 38000 out of 150000\n",
      "Iteration 38050 out of 150000\n",
      "Iteration 38100 out of 150000\n",
      "Iteration 38150 out of 150000\n",
      "Iteration 38200 out of 150000\n",
      "Iteration 38250 out of 150000\n",
      "Iteration 38300 out of 150000\n",
      "Iteration 38350 out of 150000\n",
      "Iteration 38400 out of 150000\n",
      "Iteration 38450 out of 150000\n",
      "Iteration 38500 out of 150000\n",
      "Iteration 38550 out of 150000\n",
      "Iteration 38600 out of 150000\n",
      "Iteration 38650 out of 150000\n",
      "Iteration 38700 out of 150000\n",
      "Iteration 38750 out of 150000\n",
      "Iteration 38800 out of 150000\n",
      "Iteration 38850 out of 150000\n",
      "Iteration 38900 out of 150000\n",
      "Iteration 38950 out of 150000\n",
      "Iteration 39000 out of 150000\n",
      "Iteration 39050 out of 150000\n",
      "Iteration 39100 out of 150000\n",
      "Iteration 39150 out of 150000\n",
      "Iteration 39200 out of 150000\n",
      "Iteration 39250 out of 150000\n",
      "Iteration 39300 out of 150000\n",
      "Iteration 39350 out of 150000\n",
      "Iteration 39400 out of 150000\n",
      "Iteration 39450 out of 150000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (9, 20) for Tensor 'Placeholder_1:0', which has shape '(32, 20)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-508-a34c0b72cb2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#Next Batch of reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextBatchLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnextBatchLabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#Write summary to Tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    962\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (9, 20) for Tensor 'Placeholder_1:0', which has shape '(32, 20)'"
     ]
    }
   ],
   "source": [
    "lstm_graph = tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "with lstm_graph.as_default():\n",
    "\n",
    "\n",
    "    labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "    input_data = tf.placeholder(tf.int32, [batchSize, max_tweet_length])\n",
    "    data = tf.Variable(tf.zeros([batchSize, max_tweet_length, numDimensions]),dtype=tf.float32)\n",
    "    data = tf.nn.embedding_lookup(final_embeddings, input_data)\n",
    "    # ensure the data is float32 type\n",
    "    data = tf.cast(data,tf.float32)\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "    # multiple the output of the RNN cell\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    prediction = (tf.matmul(last, weight) + bias)\n",
    "    # check accuracy during training\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    # loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    \n",
    "    # train the network\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # tensorboard\n",
    "    tf.summary.scalar('Loss', loss)\n",
    "    tf.summary.scalar('Accuracy', accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    logdir = \"tensorboard/20length/\" \n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    \n",
    "    # setup the generators\n",
    "    gen_test = batch_generator(batchSize, tweet_idxs_matrix_test, y_test_lstm)\n",
    "    gen_train = batch_generator(batchSize, tweet_idxs_matrix_train, y_train_lstm)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #Next Batch of reviews\n",
    "        nextBatch, nextBatchLabels = next(gen_train)\n",
    "        sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "        #Write summary to Tensorboard\n",
    "        if (i % 50 == 0):\n",
    "            print(f'Iteration {i} out of {iterations}')\n",
    "            summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "            writer.add_summary(summary, i)\n",
    "            \n",
    "\n",
    "        #Save the network every 10,000 training iterations\n",
    "        if (i % 10_000 == 0 and i != 0):\n",
    "            save_path = saver.save(sess, \"tensorflow/lstm_model\", global_step=i)\n",
    "            print(\"saved to %s\" % save_path)\n",
    "    writer.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_founder",
   "language": "python",
   "name": "nlp_founder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
