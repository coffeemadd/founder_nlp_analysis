{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-8ac903cb4d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from lib.ops import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "# open twitter sentiment data\n",
    "# 1 = positive\n",
    "# 0 = Negative\n",
    "twitter_data_df = pd.read_csv(\"../data/raw/Twitter_Sentiment_Analysis/Sentiment Analysis Dataset.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['..', 'Omgaga', '.', 'Im', 'sooo', 'im', 'gunna', 'CRy', '.', 'I', \"'ve\", 'been', 'at', 'this', 'dentist', 'since', '11..', 'I', 'was', 'suposed', '2', 'just', 'get', 'a', 'crown', 'put', 'on', '(', '30mins', ')', '...'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = twitter_data_df.SentimentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_all = twitter_data_df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578612, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y_all, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183959,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183959,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once this is done, we need to convert the sentences as a one-hot tensor of shape [sentence_length x word_length x alphabet_size].\n",
    "- https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_ALPHABET = 'abcdefghijklmnopqrstuvwxyz0123456789-,;' \\\n",
    "               '.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "MAX_WORD_LENGTH = 16 # number of characters in a word\n",
    "ALPHABET_SIZE = len(emb_alphabet)\n",
    "# we associate every character in our alphabet to a number: \n",
    "# e.g. b => 1 d => 3 etc.\n",
    "ALPHABET_DICT = {ch: ix for ix, ch in enumerate(emb_alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_one_hot( sentence, emb_alphabet, max_word_length, alphabet_size, alphabet_dict):\n",
    "    \"\"\"Convert a sentence to a one hot character encoding tensor for that sentence using alphanumeric characters\"\"\"\n",
    "    # https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/\n",
    "    # Convert Sentences to np.array of Shape \n",
    "    # ('sent_length', 'word_length', 'emb_size')\n",
    "\n",
    "    sent = []\n",
    "\n",
    "    # We need to keep track of the maximum length of the sentence in a minibatch\n",
    "    # so that we can pad them with zeros, this is why we return the length of every\n",
    "    # sentences after they are converted to one-hot tensors\n",
    "    SENT_LENGTH = 0\n",
    "\n",
    "    # Here, we remove any non-printable characters in a sentence (mostly\n",
    "    # non-ASCII characters)\n",
    "    printable = emb_alphabet\n",
    "    encoded_sentence = filter(lambda x: x in printable, sentence)\n",
    "\n",
    "    # word_tokenize() splits a sentence into an array where each element is\n",
    "    # a word in the sentence, for example, \n",
    "    # \"My name is Charles\" => [\"My\", \"name\", \"is\", Charles\"]\n",
    "    blob = TextBlob(sentence)\n",
    "    individual_words_from_sentence = blob.tokenize()\n",
    "    for word in individual_words_from_sentence :\n",
    "\n",
    "        # Encode one word as a matrix of shape [max_word_length x ALPHABET_SIZE]\n",
    "        word_encoding = np.zeros(shape=(max_word_length, alphabet_size))\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "\n",
    "            # If the character is not in the alphabet, ignore it    \n",
    "            try:\n",
    "                char_encoding = alphabet_dict[char]\n",
    "                one_hot = np.zeros(ALPHABET_SIZE)\n",
    "                one_hot[char_encoding] = 1\n",
    "                word_encoding[i] = one_hot\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        sent.append(np.array(word_encoding))\n",
    "        SENT_LENGTH += 1\n",
    "\n",
    "    return np.array(sent), SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "array,length = encode_one_hot(X_all[1], EMB_ALPHABET , MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT)\n",
    "# array is number of words in sentence X Max word length  X one hot encoding for alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad tensors to all have the same size \n",
    "- [batch_size x maximum_sentence_length x maximum_word_length x alphabet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   is so sad for my APL friend............'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[0][2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_minibatch( sentences, sentiment_y,  max_word_length, alphabet_size):\n",
    "    \"\"\"Create a minibath of one-hot encoded sentences \n",
    "    array is Batch Size X max number of words in sentence X Max word length  X one hot encoding for alphabet,\n",
    "    \n",
    "    and one hot encoded y [0,1] or [1,0] whre 1=positive\"\"\"\n",
    "    # Create a minibatch of sentences and convert sentiment\n",
    "    # to a one-hot vector, also takes care of padding\n",
    "\n",
    "    max_word_length = max_word_length\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    max_length = 0\n",
    "\n",
    "    for sentence,sent in zip(sentences, sentiment_y):\n",
    "        # Append the one-hot encoding of the sentiment to the minibatch of Y\n",
    "        # 0: Negative 1: Positive\n",
    "        minibatch_y.append(np.array([0, 1]) if sent == 0 else np.array([1, 0]))\n",
    "\n",
    "        # One-hot encoding of the sentence\n",
    "        one_hot, length = encode_one_hot(sentence,EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )\n",
    "\n",
    "        # Calculate maximum_sentence_length\n",
    "        if length >= max_length:\n",
    "            max_length = length\n",
    "\n",
    "        # Append encoded sentence to the minibatch of X\n",
    "        minibatch_x.append(one_hot)\n",
    "\n",
    "\n",
    "    # data is a np.array of shape ('b', 's', 'w', 'e') we want to\n",
    "    # pad it with np.zeros of shape ('e',) to get \n",
    "    # ('b', 'SENTENCE_MAX_LENGTH', 'WORD_MAX_LENGTH', 'e')\n",
    "    def numpy_fillna(data):\n",
    "        \"\"\" This is a very useful function that fill the holes in our tensor \"\"\"\n",
    "\n",
    "        # Get lengths of each row of data\n",
    "        lens = np.array([len(i) for i in data])\n",
    "\n",
    "        # Mask of valid places in each row\n",
    "        mask = np.arange(lens.max()) < lens[:, None]\n",
    "        #print(mask)\n",
    "\n",
    "        # Setup output array and put elements from data into masked positions\n",
    "        out = np.zeros(shape=(mask.shape + (max_word_length, alphabet_size)),\n",
    "                       dtype='float32')\n",
    "        #print(out)\n",
    "\n",
    "        out[mask] = np.concatenate(data)\n",
    "        #print(out,'final')\n",
    "        return out\n",
    "\n",
    "    # Padding...\n",
    "    minibatch_x = numpy_fillna(minibatch_x)\n",
    "\n",
    "    return minibatch_x, np.array(minibatch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x, minibatch_y = make_minibatch(X_all[:15],y_all[:15], MAX_WORD_LENGTH, ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 31, 16, 69)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a generator to feed to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatch(batch_size, X_data, y_data):\n",
    "        \"\"\" Returns Next Batch \"\"\"\n",
    "        \n",
    "        n_samples = len(X_data)\n",
    "        \n",
    "        # Number of batches / number of iterations per epoch\n",
    "        n_batch = int(n_samples // batch_size)\n",
    "        \n",
    "        # Creates a minibatch, loads it to RAM and feed it to the network\n",
    "        # until the buffer is empty\n",
    "        for i in range(n_batch):\n",
    "\n",
    "            inputs, targets = make_minibatch(X_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             y_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             MAX_WORD_LENGTH, ALPHABET_SIZE)\n",
    "            yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       " \n",
       " \n",
       "        [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       " \n",
       " \n",
       "        [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "          [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       " \n",
       " \n",
       "        [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       " \n",
       " \n",
       "        [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]]]], dtype=float32),\n",
       " array([[0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0]]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterate_minibatch(16,X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "    \"\"\" Straight-forward convvolutional layer \"\"\"\n",
    "    # w is the kernel, b the bias, no strides and VALID padding\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "        b = tf.get_variable('b', [output_dim])\n",
    "\n",
    "    return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdnn(input_, kernels, kernel_features, scope='TDNN'):\n",
    "    ''' Time Delay Neural Network\n",
    "    :input:           input float tensor of shape \n",
    "                      [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "    :kernels:         array of kernel sizes\n",
    "    :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "    '''\n",
    "    assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "\n",
    "    # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'embed_size') we\n",
    "    # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
    "    # use conv2D\n",
    "    # It might not seem obvious why we need to use this small hack at first sight, the reason\n",
    "    # is that sentence_length will change across the different minibatches, but if we kept it\n",
    "    # as is sentence_length would act as the number of channels in the convnet which NEEDS to\n",
    "    # stay the same\n",
    "    input_ = tf.reshape(input_, [-1, self.max_word_length, ALPHABET_SIZE])\n",
    "    input_ = tf.expand_dims(input_, 1)\n",
    "\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope):\n",
    "        for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "            reduced_length = self.max_word_length - kernel_size + 1\n",
    "\n",
    "            # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
    "            conv = conv2d(input_, kernel_feature_size, 1,\n",
    "                          kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "\n",
    "            # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
    "            pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "            layers.append(tf.squeeze(pool, [1, 2]))\n",
    "\n",
    "        if len(kernels) > 1:\n",
    "            output = tf.concat(layers, 1)\n",
    "        else:\n",
    "            output = layers[0]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 39, 16, 69)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 31, 16, 69)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 , l = encode_one_hot(X_all[1],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2 , l = encode_one_hot(X_all[2],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 16, 69)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 16, 69)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to train two models - LSTM and CNN to sentiment analysis\n",
    "- Using tweets here because the primary source of text from founders will be Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_founder",
   "language": "python",
   "name": "nlp_founder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
