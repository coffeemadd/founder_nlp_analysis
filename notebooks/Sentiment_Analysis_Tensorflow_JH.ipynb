{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "# open twitter sentiment data\n",
    "# 1 = positive\n",
    "# 0 = Negative\n",
    "twitter_data_df = pd.read_csv(\"../data/raw/Twitter_Sentiment_Analysis/Sentiment Analysis Dataset.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = twitter_data_df.SentimentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_all = twitter_data_df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578612, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t, X_val, Y_t, Y_val = train_test_split(X_all,Y_all, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_t,Y_t, test_size=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126289,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once this is done, we need to convert the sentences as a one-hot tensor of shape [sentence_length x word_length x alphabet_size].\n",
    "- https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMB_ALPHABET = 'abcdefghijklmnopqrstuvwxyz0123456789-,;' \\\n",
    "               '.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "MAX_WORD_LENGTH = 16 # number of characters in a word\n",
    "ALPHABET_SIZE = len(EMB_ALPHABET)\n",
    "# we associate every character in our alphabet to a number: \n",
    "# e.g. b => 1 d => 3 etc.\n",
    "ALPHABET_DICT = {ch: ix for ix, ch in enumerate(EMB_ALPHABET)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_one_hot( sentence, emb_alphabet, max_word_length, alphabet_size, alphabet_dict):\n",
    "    \"\"\"Convert a sentence to a one hot character encoding tensor for that sentence using alphanumeric characters\"\"\"\n",
    "    # https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/\n",
    "    # Convert Sentences to np.array of Shape \n",
    "    # ('sent_length', 'word_length', 'emb_size')\n",
    "\n",
    "    sent = []\n",
    "\n",
    "    # We need to keep track of the maximum length of the sentence in a minibatch\n",
    "    # so that we can pad them with zeros, this is why we return the length of every\n",
    "    # sentences after they are converted to one-hot tensors\n",
    "    SENT_LENGTH = 0\n",
    "\n",
    "    # Here, we remove any non-printable characters in a sentence (mostly\n",
    "    # non-ASCII characters)\n",
    "    printable = emb_alphabet\n",
    "    encoded_sentence = filter(lambda x: x in printable, sentence)\n",
    "\n",
    "    # word_tokenize() splits a sentence into an array where each element is\n",
    "    # a word in the sentence, for example, \n",
    "    # \"My name is Charles\" => [\"My\", \"name\", \"is\", Charles\"]\n",
    "    blob = TextBlob(sentence)\n",
    "    individual_words_from_sentence = blob.tokenize()\n",
    "    for word in individual_words_from_sentence :\n",
    "\n",
    "        # Encode one word as a matrix of shape [max_word_length x ALPHABET_SIZE]\n",
    "        word_encoding = np.zeros(shape=(max_word_length, alphabet_size))\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "\n",
    "            # If the character is not in the alphabet, ignore it    \n",
    "            try:\n",
    "                char_encoding = alphabet_dict[char]\n",
    "                one_hot = np.zeros(ALPHABET_SIZE)\n",
    "                one_hot[char_encoding] = 1\n",
    "                word_encoding[i] = one_hot\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        sent.append(np.array(word_encoding))\n",
    "        SENT_LENGTH += 1\n",
    "\n",
    "    return np.array(sent), SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array,length = encode_one_hot(X_all[1], EMB_ALPHABET , MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT)\n",
    "# array is number of words in sentence X Max word length  X one hot encoding for alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad tensors to all have the same size \n",
    "- [batch_size x maximum_sentence_length x maximum_word_length x alphabet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   is so sad for my APL friend............'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[0][2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_minibatch( sentences, sentiment_y,  max_word_length, alphabet_size):\n",
    "    \"\"\"Create a minibath of one-hot encoded sentences \n",
    "    array is Batch Size X max number of words in sentence X Max word length  X one hot encoding for alphabet,\n",
    "    \n",
    "    and one hot encoded y [0,1] or [1,0] whre 1=positive\"\"\"\n",
    "    # Create a minibatch of sentences and convert sentiment\n",
    "    # to a one-hot vector, also takes care of padding\n",
    "\n",
    "    max_word_length = max_word_length\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    max_length = 0\n",
    "\n",
    "    for sentence,sent in zip(sentences, sentiment_y):\n",
    "        # Append the one-hot encoding of the sentiment to the minibatch of Y\n",
    "        # 0: Negative 1: Positive\n",
    "        minibatch_y.append(np.array([0, 1]) if sent == 0 else np.array([1, 0]))\n",
    "\n",
    "        # One-hot encoding of the sentence\n",
    "        one_hot, length = encode_one_hot(sentence,EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )\n",
    "\n",
    "        # Calculate maximum_sentence_length\n",
    "        if length >= max_length:\n",
    "            max_length = length\n",
    "\n",
    "        # Append encoded sentence to the minibatch of X\n",
    "        minibatch_x.append(one_hot)\n",
    "\n",
    "\n",
    "    # data is a np.array of shape ('b', 's', 'w', 'e') we want to\n",
    "    # pad it with np.zeros of shape ('e',) to get \n",
    "    # ('b', 'SENTENCE_MAX_LENGTH', 'WORD_MAX_LENGTH', 'e')\n",
    "    def numpy_fillna(data):\n",
    "        \"\"\" This is a very useful function that fill the holes in our tensor \"\"\"\n",
    "\n",
    "        # Get lengths of each row of data\n",
    "        lens = np.array([len(i) for i in data])\n",
    "\n",
    "        # Mask of valid places in each row\n",
    "        mask = np.arange(lens.max()) < lens[:, None]\n",
    "        #print(mask)\n",
    "\n",
    "        # Setup output array and put elements from data into masked positions\n",
    "        out = np.zeros(shape=(mask.shape + (max_word_length, alphabet_size)),\n",
    "                       dtype='float32')\n",
    "        #print(out)\n",
    "\n",
    "        out[mask] = np.concatenate(data)\n",
    "        #print(out,'final')\n",
    "        return out\n",
    "\n",
    "    # Padding...\n",
    "    minibatch_x = numpy_fillna(minibatch_x)\n",
    "\n",
    "    return minibatch_x, np.array(minibatch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x, minibatch_y = make_minibatch(X_test[:15],Y_test[:15], MAX_WORD_LENGTH, ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 36, 16, 69)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a generator to feed to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatch(batch_size, X_data, y_data):\n",
    "        \"\"\" Returns Next Batch \"\"\"\n",
    "        \n",
    "        n_samples = len(X_data)\n",
    "        \n",
    "        # Number of batches / number of iterations per epoch\n",
    "        n_batch = int(n_samples // batch_size)\n",
    "        \n",
    "        # Creates a minibatch, loads it to RAM and feed it to the network\n",
    "        # until the buffer is empty\n",
    "        for i in range(n_batch):\n",
    "\n",
    "            inputs, targets = make_minibatch(X_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             y_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             MAX_WORD_LENGTH, ALPHABET_SIZE)\n",
    "            yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "    \"\"\" Straight-forward convvolutional layer \"\"\"\n",
    "    # w is the kernel, b the bias, no strides and VALID padding\n",
    "    \n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "            b = tf.get_variable('b', [output_dim])\n",
    "\n",
    "        return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tdnn(input_, kernels, kernel_features, ALPHABET_SIZE, scope='TDNN'):\n",
    "    ''' Time Delay Neural Network\n",
    "    :input:           input float tensor of shape \n",
    "                      [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "    :kernels:         array of kernel sizes\n",
    "    :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "    '''\n",
    "    assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "\n",
    "    # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'embed_size') we\n",
    "    # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
    "    # use conv2D\n",
    "    # It might not seem obvious why we need to use this small hack at first sight, the reason\n",
    "    # is that sentence_length will change across the different minibatches, but if we kept it\n",
    "    # as is sentence_length would act as the number of channels in the convnet which NEEDS to\n",
    "    # stay the same\n",
    "    input_ = tf.reshape(input_, [-1, MAX_WORD_LENGTH, ALPHABET_SIZE])\n",
    "    input_ = tf.expand_dims(input_, 1)\n",
    "\n",
    "    layers = []\n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "        with tf.variable_scope(scope):\n",
    "            for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "                reduced_length = MAX_WORD_LENGTH - kernel_size + 1\n",
    "\n",
    "                # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
    "                conv = conv2d(input_, kernel_feature_size, 1,\n",
    "                              kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "\n",
    "                # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
    "                pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "                layers.append(tf.squeeze(pool, [1, 2]))\n",
    "\n",
    "            if len(kernels) > 1:\n",
    "                output = tf.concat(layers, 1)\n",
    "            else:\n",
    "                output = layers[0]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This goes in ops.py\n",
    "\n",
    "def linear(input_, output_size, scope=None):\n",
    "    \"\"\"\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "    \n",
    "    Args:\n",
    "        args: a tensor or a list of 2D, batch x n, Tensors.\n",
    "        output_size: int, second dimension of W[i].\n",
    "        scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "        \n",
    "    Returns:\n",
    "        A 2D Tensor with shape [batch x output_size] equal to\n",
    "        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        # Now the computation.\n",
    "        with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "            matrix = tf.get_variable(\"Matrix\", [output_size, input_size],\n",
    "                                     dtype=input_.dtype)\n",
    "            bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway network to help train deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            for idx in range(num_layers):\n",
    "                g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "                t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "                output = t * g + (1. - t) * input_\n",
    "                input_ = output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(object):\n",
    "    \"\"\" Character-Level LSTM Implementation \"\"\"\n",
    "\n",
    "    def __init__(self, hparams=None):\n",
    "        # Get the hyperparameters\n",
    "        self.hparams = self.get_hparams()\n",
    "        g = tf.get_default_graph()\n",
    "        with g.as_default():\n",
    "        \n",
    "            # maximum length of each words\n",
    "            max_word_length = self.hparams['max_word_length']\n",
    "\n",
    "            # X is of shape ('b', 'sentence_length', 'max_word_length', 'alphabet_size']\n",
    "            # Placeholder for the one-hot encoded sentences\n",
    "            X = tf.placeholder('float32', \n",
    "                                    shape=[None, None, max_word_length, ALPHABET_SIZE],\n",
    "                                    name='X')\n",
    "            self.X = X\n",
    "\n",
    "            # Placeholder for the one-hot encoded sentiment\n",
    "            Y = tf.placeholder('float32', shape=[None, 2], name='Y')\n",
    "            self.Y = Y\n",
    "        \n",
    "    def get_hparams(self):\n",
    "        ''' Get Hyperparameters '''\n",
    "\n",
    "        return {\n",
    "            'BATCH_SIZE':       64,\n",
    "            'EPOCHS':           500,\n",
    "            'max_word_length':  16,\n",
    "            'learning_rate':    0.0001,\n",
    "            'patience':         10000,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = lstm.X # tf placeholder \n",
    "Y = lstm.Y # tf placeholder for class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input_, out_dim, scope=None):\n",
    "    \"\"\" SoftMax Output \"\"\"\n",
    "    \n",
    "    g = tf.get_default_graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.variable_scope(scope or 'softmax'):\n",
    "            W = tf.get_variable('W', [input_.get_shape()[1], out_dim])\n",
    "            b = tf.get_variable('b', [out_dim])\n",
    "\n",
    "    return tf.nn.softmax(tf.matmul(input_, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136600,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126289,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = None # class variable\n",
    "train_samples=1136600\n",
    "valid_samples = 126289\n",
    "def build(\n",
    "              training=True,\n",
    "              testing_batch_size=1000,\n",
    "              kernels=[1, 2, 3, 4, 5, 6, 7],\n",
    "              kernel_features=[25, 50, 75, 100, 125, 150, 175],\n",
    "              rnn_size=650,\n",
    "              dropout=0.0,\n",
    "              size=700,\n",
    "              train_samples=train_samples,\n",
    "              valid_samples=valid_samples,\n",
    "              hparams= lstm.get_hparams()):\n",
    "        \"\"\"\n",
    "        Build the computational graph\n",
    "        \n",
    "        :param training: \n",
    "            Boolean whether we are training (True) or testing (False)\n",
    "        \n",
    "        :param testing_batch_size: \n",
    "            Batch size to use during testing \n",
    "            \n",
    "        :param kernels:\n",
    "            Kernel width for each convolutional layer\n",
    "         \n",
    "        :param kernel_features: \n",
    "            Number of kernels for each convolutional layer\n",
    "            \n",
    "        :param rnn_size: \n",
    "            Size of the LSTM output\n",
    "        \n",
    "        :param dropout: \n",
    "            Retain probability when using dropout\n",
    "        \n",
    "        :param size: \n",
    "            Size of the Highway embeding\n",
    "        \n",
    "        :param train_samples: \n",
    "            Number of training samples\n",
    "        \n",
    "        :param valid_samples: \n",
    "            Number of validation samples\n",
    "        :hyperparameters\n",
    "            dict of parameters\n",
    "        \"\"\"\n",
    "\n",
    "        max_word_length = hparams['max_word_length']\n",
    "        size = size\n",
    "        max_word_length = hparams['max_word_length']\n",
    "        train_samples = train_samples\n",
    "        valid_samples = valid_samples\n",
    "\n",
    "        \n",
    "        # If we are training use the BATCH_SIZE from the hyperparameters\n",
    "        # else use the testing batch size\n",
    "        if training == True:\n",
    "            BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "\n",
    "        else:\n",
    "            BATCH_SIZE = testing_batch_size\n",
    "            \n",
    "       \n",
    "\n",
    "        \n",
    "        # Pass the sentences through the CharCNN network\n",
    "        cnn = tdnn(X, kernels, kernel_features, ALPHABET_SIZE) # X is a TF placeholder from the LSTM network\n",
    "\n",
    "        # tdnn() returns a tensor of shape [batch_size * sentence_length x kernel_features]\n",
    "        # highway() returns a tensor of shape [batch_size * sentence_length x size] to use\n",
    "        # tensorflow dynamic_rnn module we need to reshape it to \n",
    "        # [batch_size x sentence_length x size]\n",
    "        cnn = highway(cnn, size)\n",
    "\n",
    "        cnn = tf.reshape(cnn, [BATCH_SIZE, -1, size])\n",
    "        # deafult graph\n",
    "        g = tf.get_default_graph()\n",
    "        with g.as_default():\n",
    "        \n",
    "            # Build the LSTM\n",
    "            with tf.variable_scope('LSTM'):\n",
    "\n",
    "                # The following is pretty straight-forward, create a cell and add dropout if\n",
    "                # necessary. Note that I did not use dropout to get my results, but using it\n",
    "                # will probably help\n",
    "                def create_rnn_cell():\n",
    "                    cell = rnn.BasicLSTMCell(rnn_size, state_is_tuple=True,\n",
    "                                             forget_bias=0.0, reuse=False)\n",
    "\n",
    "                    if dropout > 0.0:\n",
    "                        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1. - dropout)\n",
    "\n",
    "                    return cell\n",
    "\n",
    "                cell = create_rnn_cell()\n",
    "\n",
    "                # Initial state of the LSTM cell\n",
    "                initial_rnn_state = cell.zero_state(BATCH_SIZE, dtype='float32')\n",
    "\n",
    "                # This function returns the outputs at every steps of \n",
    "                # the LSTM (i.e. one output for every word)\n",
    "                outputs, final_rnn_state = tf.nn.dynamic_rnn(cell, cnn,\n",
    "                                                             initial_state=initial_rnn_state,\n",
    "                                                             dtype=tf.float32)\n",
    "\n",
    "\n",
    "                # In this implementation, we only care about the last outputs of the RNN\n",
    "                # i.e. the output at the end of the sentence\n",
    "                outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "                last = outputs[-1]\n",
    "            global prediction\n",
    "            prediction = softmax(last, 2)\n",
    "        \n",
    "\n",
    "\n",
    "size = None\n",
    "hparams = None\n",
    "max_word_length = None\n",
    "\n",
    "\n",
    "build(hparams=lstm.get_hparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(64, 2) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PATH = '/NOBACKUP/ashbylepoc/PycharmProjects/CharLSTM/'\n",
    "# TRAIN_SET = PATH + 'datasets/train_set.csv'\n",
    "# TEST_SET = PATH + 'datasets/test_set.csv'\n",
    "# VALID_SET = PATH + 'datasets/valid_set.csv'\n",
    "\n",
    "SAVE_PATH = 'tensorflow/lstm'\n",
    "LOGGING_PATH = 'tensorflow/log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(hparams, X_train, Y_train, X_test, Y_test, load_model = False):\n",
    "    \n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "    EPOCHS = hparams['EPOCHS']\n",
    "    max_word_length = hparams['max_word_length']\n",
    "    learning_rate = hparams['learning_rate']\n",
    "\n",
    "    # the probability for each sentiment (pos, neg)\n",
    "    pred = prediction\n",
    "\n",
    "    # Binary cross-entropy loss\n",
    "    cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "    # The number of \"predictions\" we got right, we assign a sentence with \n",
    "    # a positive connotation when the probability to be positive is greater then\n",
    "    # the probability of being negative and vice-versa.\n",
    "    predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    # Accuracy: # predictions right / total number of predictions\n",
    "    acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "    # We use the Adam Optimizer\n",
    "    \n",
    "    try: # see if this optimizer is already defined\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cost)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "    n_batch = train_samples // BATCH_SIZE # train samples is class variable\n",
    "\n",
    "    # parameters for saving and early stopping\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    patience = hparams['patience']\n",
    "    \n",
    "    \n",
    "   # tf.add_to_collection('train_op', train_op) ## for the variables to be restores\n",
    "    \n",
    "    # get graph\n",
    "    g = tf.get_default_graph()\n",
    "\n",
    "\n",
    "    with tf.Session(graph = g) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        best_acc = 0.0\n",
    "        DONE = False\n",
    "        epoch = 0\n",
    "        g_step = 0\n",
    "        if load_model == True:\n",
    "            saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "            all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "            for v in all_vars:\n",
    "                v_ = sess.run(v)\n",
    "                print(v_)\n",
    "            print(\"Finished loading model\")\n",
    "\n",
    "        while epoch <= EPOCHS and not DONE:\n",
    "            loss = 0.0\n",
    "            batch = 1\n",
    "            epoch += 1\n",
    "\n",
    "#             with open(TRAIN_SET, 'r') as f:\n",
    "#                 # Load the file in a TextReader object until we've read it all\n",
    "#                 # then create a new TextReader Object for how many epochs we \n",
    "#                 # want to loop on\n",
    "#                 reader = TextReader(f, max_word_length)\n",
    "                \n",
    "            for minibatch in iterate_minibatch(BATCH_SIZE, X_train, Y_train):\n",
    "                g_step +=1\n",
    "                batch_x, batch_y = minibatch\n",
    "\n",
    "                # Do backprop and compute the cost and accuracy on this minibatch\n",
    "                _, c, a = sess.run([optimizer, cost, acc],\n",
    "                                   feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "                loss += c\n",
    "\n",
    "                if batch % 10 == 0: # change back to 100\n",
    "                    # Compute Accuracy on the Training set and print some info\n",
    "                    print('Epoch: %5d/%5d -- batch: %5d/%5d -- Loss: %.4f -- Train Accuracy: %.4f -- Patience %.4f' %\n",
    "                          (epoch, EPOCHS, batch, n_batch, loss/batch, a, patience))\n",
    "\n",
    "                    # Write loss and accuracy to some file\n",
    "                    log = open(LOGGING_PATH, 'a')\n",
    "                    log.write('%s, %6d, %.5f, %.5f \\n' % ('train', epoch * batch, loss/batch, a))\n",
    "                    log.close()\n",
    "                    \n",
    "\n",
    "                # --------------\n",
    "                # EARLY STOPPING\n",
    "                # --------------\n",
    "\n",
    "                # Compute Accuracy on the Validation set, check if validation has improved, save model, etc\n",
    "                if batch % 500 == 0:\n",
    "                    m = 1\n",
    "                    accuracy = []\n",
    "\n",
    "                    # Validation set is very large, so accuracy is computed on testing set\n",
    "                    # instead of valid set, change TEST_SET to VALID_SET to compute accuracy on valid set\n",
    "#                     with open(TEST_SET, 'r') as ff:\n",
    "#                         valid_reader = TextReader(ff, max_word_length)\n",
    "                    for mb in iterate_minibatch(BATCH_SIZE, X_test, Y_test):\n",
    "                        valid_x, valid_y = mb\n",
    "                        a = sess.run([acc], feed_dict={X: valid_x, Y: valid_y}) # X and Y are placeholder nodes\n",
    "                        accuracy.append(a)\n",
    "                        m +=1\n",
    "                        if m == 10: # to speed up training only go through 10 batches \n",
    "                            break\n",
    " \n",
    "                    mean_acc = np.mean(accuracy)\n",
    "\n",
    "                    # if accuracy has improved, save model and boost patience\n",
    "                    if mean_acc > best_acc:\n",
    "                        best_acc = mean_acc\n",
    "                        save_path = saver.save(sess, SAVE_PATH, global_step=g_step)\n",
    "                        patience = hparams['patience']\n",
    "                        print('Model saved in file: %s' % save_path)\n",
    "\n",
    "                    # else reduce patience and break loop if necessary\n",
    "                    else:\n",
    "                        patience -= 500\n",
    "                        if patience <= 0:\n",
    "                            DONE = True\n",
    "                            break\n",
    "\n",
    "                    print('Epoch: %5d/%5d -- batch: %5d/%5d -- Valid Accuracy: %.4f' %\n",
    "                         (epoch, EPOCHS, batch, n_batch, mean_acc))\n",
    "\n",
    "                    # Write validation accuracy to log file\n",
    "                    log = open(LOGGING_PATH, 'a')\n",
    "                    log.write('%s, %6d, %.5f \\n' % ('valid', epoch * batch, mean_acc))\n",
    "                    log.close()\n",
    "\n",
    "                batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(lstm.get_hparams(),X_train, Y_train, X_test, Y_test, load_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_test_set(hparams, X_val, Y_val):\n",
    "        \"\"\"\n",
    "        Evaluate Test Set\n",
    "        On a model that trained for around 5 epochs it achieved:\n",
    "        # Valid loss: 23.50035 -- Valid Accuracy: 0.83613\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = hparams['BATCH_SIZE']\n",
    "        max_word_length = hparams['max_word_length']\n",
    "\n",
    "        pred = prediction\n",
    "        \n",
    "        cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "        acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "        # restart the graph\n",
    "        tf.reset_default_graph()\n",
    "        # parameters for restoring variables\n",
    "        \n",
    "    \n",
    "        #saver = tf.train.Saver()\n",
    "        #saver = tf.train.import_meta_graph('./tensorflow/lstm.meta')\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            print('Loading model %s...' % SAVE_PATH)\n",
    "            saver = tf.train.import_meta_graph('./tensorflow/lstm.meta')\n",
    "            var = tf.global_variables()[0]\n",
    "            print(var)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            \n",
    "            saver.restore(sess, SAVE_PATH)\n",
    "            print('Done!')\n",
    "            loss = []\n",
    "            accuracy = []\n",
    "\n",
    "#             with open(VALID_SET, 'r') as f:\n",
    "#                 reader = TextReader(f, max_word_length)\n",
    "            for minibatch in iterate_minibatch(BATCH_SIZE, X_val, Y_val):\n",
    "                batch_x, batch_y = minibatch\n",
    "\n",
    "                c, a = sess.run([cost, acc], feed_dict={X: batch_x, Y: batch_y}) # X and Y are placeholder variables\n",
    "                loss.append(c)\n",
    "                accuracy.append(a)\n",
    "\n",
    "            loss = np.mean(loss)\n",
    "            accuracy = np.mean(accuracy)\n",
    "            print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "            return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_test_set(lstm.get_hparams(),X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tensorflow/lstm\n",
      "Finished loading model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        best_acc = 0.0\n",
    "        DONE = False\n",
    "        epoch = 0\n",
    "        g_step = 0\n",
    "        #if load_model == True:\n",
    "        saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "        all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "        for v in all_vars:\n",
    "            v_ = sess.run(v)\n",
    "            print(v_)\n",
    "        print(\"Finished loading model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model tensorflow/lstm...\n",
      "INFO:tensorflow:Restoring parameters from ./tensorflow/lstm\n",
      "Done!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_5:0\", shape=(64, 2), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-4f7d979b08e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keep_dims, name, reduction_indices)\u001b[0m\n\u001b[1;32m   1234\u001b[0m       \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, reduction_indices, keep_dims, name)\u001b[0m\n\u001b[1;32m   2654\u001b[0m   result = _op_def_lib.apply_op(\"Sum\", input=input,\n\u001b[1;32m   2655\u001b[0m                                 \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m                                 keep_dims=keep_dims, name=name)\n\u001b[0m\u001b[1;32m   2657\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m       \u001b[0;31m# pyline: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3849\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3851\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_5:0\", shape=(64, 2), dtype=float32)."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('Loading model %s...' % SAVE_PATH)\n",
    "    saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "    all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "    for v in all_vars:\n",
    "        v_ = sess.run(v)\n",
    "        print(v_,'Var')\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    pred = prediction\n",
    "\n",
    "\n",
    "    cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "    predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "        \n",
    "    loss = []\n",
    "    accuracy = []\n",
    "        \n",
    "    for minibatch in iterate_minibatch(BATCH_SIZE, X_val, Y_val):\n",
    "            batch_x, batch_y = minibatch\n",
    "\n",
    "            c, a = sess.run([cost, acc], feed_dict={X: batch_x, Y: batch_y}) # X and Y are placeholder variables\n",
    "            loss.append(c)\n",
    "            accuracy.append(a)\n",
    "\n",
    "    loss = np.mean(loss)\n",
    "    accuracy = np.mean(accuracy)\n",
    "    print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "    print(loss, accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model tensorflow/lstm...\n",
      "INFO:tensorflow:Restoring parameters from ./tensorflow/lstm\n",
      "Finished loading model\n",
      "Done!\n",
      "<tensorflow.python.framework.ops.Graph object at 0x11a6441d0> graph\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_7:0\", shape=(64, 2), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ef6f2b006261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keep_dims, name, reduction_indices)\u001b[0m\n\u001b[1;32m   1234\u001b[0m       \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, reduction_indices, keep_dims, name)\u001b[0m\n\u001b[1;32m   2654\u001b[0m   result = _op_def_lib.apply_op(\"Sum\", input=input,\n\u001b[1;32m   2655\u001b[0m                                 \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m                                 keep_dims=keep_dims, name=name)\n\u001b[0m\u001b[1;32m   2657\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m       \u001b[0;31m# pyline: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3849\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3851\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const_6:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"mul_7:0\", shape=(64, 2), dtype=float32)."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE =64 #hparams['BATCH_SIZE']\n",
    "max_word_length = 16#hparams['max_word_length']\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('Loading model %s...' % SAVE_PATH)\n",
    "    saver = tf.train.import_meta_graph('./tensorflow/lstm.meta') #.mets is the model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./tensorflow/')) # directory where model was saved\n",
    "    all_vars = tf.get_collection('vars') # restore all vars in the graph\n",
    "    for v in all_vars:\n",
    "        v_ = sess.run(v)\n",
    "        print(v_,'Var')\n",
    "    print(\"Finished loading model\")\n",
    "    print('Done!')\n",
    "    g = tf.get_default_graph()\n",
    "    print(g,'graph')\n",
    "    with g.as_default():\n",
    "\n",
    "        pred = prediction\n",
    "\n",
    "        cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "        acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "        # # restart the graph\n",
    "        # tf.reset_default_graph()\n",
    "        # parameters for restoring variables\n",
    "\n",
    "        #saver = tf.train.Saver()\n",
    "        #saver = tf.train.import_meta_graph('./tensorflow/lstm.meta')\n",
    "\n",
    "\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "\n",
    "        #             with open(VALID_SET, 'r') as f:\n",
    "        #                 reader = TextReader(f, max_word_length)\n",
    "        for minibatch in iterate_minibatch(BATCH_SIZE, X_val, Y_val):\n",
    "            batch_x, batch_y = minibatch\n",
    "\n",
    "            c, a = sess.run([cost, acc], feed_dict={X: batch_x, Y: batch_y}) # X and Y are placeholder variables\n",
    "            loss.append(c)\n",
    "            accuracy.append(a)\n",
    "\n",
    "        loss = np.mean(loss)\n",
    "        accuracy = np.mean(accuracy)\n",
    "        print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f8caf16cb8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1 , l = encode_one_hot(X_all[1],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2 , l = encode_one_hot(X_all[2],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to train two models - LSTM and CNN to sentiment analysis\n",
    "- Using tweets here because the primary source of text from founders will be Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try creating a toy model\n",
    "- save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Attempt\n",
    "- Use multilayer perceptron  with count vectorizer to predict sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                     is so sad for my APL friend.............'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "\n",
    "X_down = X_all[:80_000]\n",
    "Y_down = Y_all[:80_000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_down = [[1,0] if i == 1 else [0,1] for i in Y_down]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_features = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=m_features)\n",
    "# try also TFIDF\n",
    "tfidf_vec = TfidfVectorizer(max_features=m_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_cvectors = count_vec.fit_transform(X_down)\n",
    "X_cvectors = tfidf_vec.fit_transform(X_down)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense =  np.array(X_cvectors.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, Y_down, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.shuffle(X_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size,X,Y):\n",
    "    \"\"\"Return a generator of batches for X and Y data\"\"\"\n",
    "    # shuffle the xamples\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(Y)\n",
    "    \n",
    "    for i in range(0,len(X),batch_size):\n",
    "        yield X[i:i+batch_size], np.array(Y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator(16, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 10000)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron for sentiment analysis first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = m_features # max features\n",
    "n_classes = 2 # sentiment (0 = negative, 1 = positive)\n",
    "\n",
    "\n",
    "\n",
    "# save path\n",
    "SAVE_PATH = 'tensorflow/MLP_sentiment_tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "\n",
    "#### maybe change the loss here to binary cross entropy \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    " # Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 7.191715240\n",
      "Average cost = 17.7361428442\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0011 cost= 0.000024024\n",
      "Average cost = 0.100141660096\n",
      "Accuracy: 0.75\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0021 cost= 0.000014840\n",
      "Average cost = 0.0477874065152\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0031 cost= 0.017330704\n",
      "Average cost = 0.0283245384775\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0041 cost= 0.000000000\n",
      "Average cost = 0.0159261162408\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0051 cost= 0.000000000\n",
      "Average cost = 0.0496339174848\n",
      "Accuracy: 0.625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0061 cost= 0.000000000\n",
      "Average cost = 0.0208531918696\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0071 cost= 0.000000000\n",
      "Average cost = 0.0238386985889\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-a6ff86f919c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m            \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n\u001b[0;32m---> 30\u001b[0;31m                                                          y: batch_y})\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m            \u001b[0;31m#a = sess.run([acc], feed_dict={x:test_x, y:test_y})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanhilgart/anaconda/envs/nlp_founder/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Initial training session\n",
    "    \n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # accuracy calculation\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))\n",
    "    \n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # set up the training generators\n",
    "        gen_train = batch_generator(16, X_train, y_train)\n",
    "        gen_test = batch_generator(16,X_test, y_test)\n",
    "    \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(X_train)/batch_size)\n",
    "      \n",
    "\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next(gen_train)\n",
    "            test_x, test_y = next(gen_test)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            \n",
    "            #a = sess.run([acc], feed_dict={x:test_x, y:test_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print( \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(c))\n",
    "            print(\"Average cost =\",avg_cost)\n",
    "            print(\"Accuracy:\", acc.eval({x: test_x, y: test_y}))\n",
    "\n",
    "             #Save model weights to disk\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess=sess, save_path = SAVE_PATH)\n",
    "            print(\"Model saved in file: %s\" % SAVE_PATH)\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from tensorflow/MLP_sentiment_tfidf\n",
      "Model restored from file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0001 cost= 0.649929523\n",
      "Average cost = 0.802235646699\n",
      "Accuracy: 0.625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0011 cost= 0.721905410\n",
      "Average cost = 0.6874845133\n",
      "Accuracy: 0.1875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0021 cost= 0.725265265\n",
      "Average cost = 0.692402385557\n",
      "Accuracy: 0.625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0031 cost= 0.667422533\n",
      "Average cost = 0.687104510955\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0041 cost= 0.643501639\n",
      "Average cost = 0.686297419744\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0051 cost= 0.685555041\n",
      "Average cost = 0.687660418833\n",
      "Accuracy: 0.5625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0061 cost= 0.715443909\n",
      "Average cost = 0.688916435417\n",
      "Accuracy: 0.375\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0071 cost= 0.686864257\n",
      "Average cost = 0.688568626459\n",
      "Accuracy: 0.625\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0081 cost= 0.673778653\n",
      "Average cost = 0.68970883141\n",
      "Accuracy: 0.5\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Epoch: 0091 cost= 0.609447837\n",
      "Average cost = 0.683865956198\n",
      "Accuracy: 0.6875\n",
      "Model saved in file: tensorflow/MLP_sentiment_tfidf\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Load an existing model\n",
    "tf.reset_default_graph()\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "    \n",
    "    \n",
    "    ## Create the graph\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "        # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "\n",
    "    #### maybe change the loss here to binary cross entropy \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver = tf.train.Saver()\n",
    "    load_path = saver.restore(sess, SAVE_PATH)\n",
    "    print(\"Model restored from file: %s\" % SAVE_PATH)\n",
    "\n",
    "    # accuracy calculation\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # set up the training generators\n",
    "        gen_train = batch_generator(16, X_train, y_train)\n",
    "        gen_test = batch_generator(16,X_test, y_test)\n",
    "    \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(X_train)/batch_size)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next(gen_train)\n",
    "            test_x, test_y = next(gen_test)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            \n",
    "            #a = sess.run([acc], feed_dict={x:test_x, y:test_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print( \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(c))\n",
    "            print(\"Average cost =\",avg_cost)\n",
    "            print(\"Accuracy:\", acc.eval({x: test_x, y: test_y}))\n",
    "\n",
    "             #Save model weights to disk\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess=sess, save_path = SAVE_PATH)\n",
    "            print(\"Model saved in file: %s\" % SAVE_PATH)\n",
    "            \n",
    "            \n",
    "    print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_founder",
   "language": "python",
   "name": "nlp_founder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
