{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "# open twitter sentiment data\n",
    "# 1 = positive\n",
    "# 0 = Negative\n",
    "twitter_data_df = pd.read_csv(\"../data/raw/Twitter_Sentiment_Analysis/Sentiment Analysis Dataset.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['..', 'Omgaga', '.', 'Im', 'sooo', 'im', 'gunna', 'CRy', '.', 'I', \"'ve\", 'been', 'at', 'this', 'dentist', 'since', '11..', 'I', 'was', 'suposed', '2', 'just', 'get', 'a', 'crown', 'put', 'on', '(', '30mins', ')', '...'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = twitter_data_df.SentimentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_all = twitter_data_df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578612, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y_all, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183959,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183959,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once this is done, we need to convert the sentences as a one-hot tensor of shape [sentence_length x word_length x alphabet_size].\n",
    "- https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMB_ALPHABET = 'abcdefghijklmnopqrstuvwxyz0123456789-,;' \\\n",
    "               '.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "MAX_WORD_LENGTH = 16 # number of characters in a word\n",
    "ALPHABET_SIZE = len(emb_alphabet)\n",
    "# we associate every character in our alphabet to a number: \n",
    "# e.g. b => 1 d => 3 etc.\n",
    "ALPHABET_DICT = {ch: ix for ix, ch in enumerate(emb_alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_one_hot( sentence, emb_alphabet, max_word_length, alphabet_size, alphabet_dict):\n",
    "    \"\"\"Convert a sentence to a one hot character encoding tensor for that sentence using alphanumeric characters\"\"\"\n",
    "    # https://charlesashby.github.io/2017/06/05/sentiment-analysis-with-char-lstm/\n",
    "    # Convert Sentences to np.array of Shape \n",
    "    # ('sent_length', 'word_length', 'emb_size')\n",
    "\n",
    "    sent = []\n",
    "\n",
    "    # We need to keep track of the maximum length of the sentence in a minibatch\n",
    "    # so that we can pad them with zeros, this is why we return the length of every\n",
    "    # sentences after they are converted to one-hot tensors\n",
    "    SENT_LENGTH = 0\n",
    "\n",
    "    # Here, we remove any non-printable characters in a sentence (mostly\n",
    "    # non-ASCII characters)\n",
    "    printable = emb_alphabet\n",
    "    encoded_sentence = filter(lambda x: x in printable, sentence)\n",
    "\n",
    "    # word_tokenize() splits a sentence into an array where each element is\n",
    "    # a word in the sentence, for example, \n",
    "    # \"My name is Charles\" => [\"My\", \"name\", \"is\", Charles\"]\n",
    "    blob = TextBlob(sentence)\n",
    "    individual_words_from_sentence = blob.tokenize()\n",
    "    for word in individual_words_from_sentence :\n",
    "\n",
    "        # Encode one word as a matrix of shape [max_word_length x ALPHABET_SIZE]\n",
    "        word_encoding = np.zeros(shape=(max_word_length, alphabet_size))\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "\n",
    "            # If the character is not in the alphabet, ignore it    \n",
    "            try:\n",
    "                char_encoding = alphabet_dict[char]\n",
    "                one_hot = np.zeros(ALPHABET_SIZE)\n",
    "                one_hot[char_encoding] = 1\n",
    "                word_encoding[i] = one_hot\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        sent.append(np.array(word_encoding))\n",
    "        SENT_LENGTH += 1\n",
    "\n",
    "    return np.array(sent), SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array,length = encode_one_hot(X_all[1], EMB_ALPHABET , MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT)\n",
    "# array is number of words in sentence X Max word length  X one hot encoding for alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad tensors to all have the same size \n",
    "- [batch_size x maximum_sentence_length x maximum_word_length x alphabet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   is so sad for my APL friend............'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all[0][2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_minibatch( sentences, sentiment_y,  max_word_length, alphabet_size):\n",
    "    \"\"\"Create a minibath of one-hot encoded sentences \n",
    "    array is Batch Size X max number of words in sentence X Max word length  X one hot encoding for alphabet,\n",
    "    \n",
    "    and one hot encoded y [0,1] or [1,0] whre 1=positive\"\"\"\n",
    "    # Create a minibatch of sentences and convert sentiment\n",
    "    # to a one-hot vector, also takes care of padding\n",
    "\n",
    "    max_word_length = max_word_length\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    max_length = 0\n",
    "\n",
    "    for sentence,sent in zip(sentences, sentiment_y):\n",
    "        # Append the one-hot encoding of the sentiment to the minibatch of Y\n",
    "        # 0: Negative 1: Positive\n",
    "        minibatch_y.append(np.array([0, 1]) if sent == 0 else np.array([1, 0]))\n",
    "\n",
    "        # One-hot encoding of the sentence\n",
    "        one_hot, length = encode_one_hot(sentence,EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )\n",
    "\n",
    "        # Calculate maximum_sentence_length\n",
    "        if length >= max_length:\n",
    "            max_length = length\n",
    "\n",
    "        # Append encoded sentence to the minibatch of X\n",
    "        minibatch_x.append(one_hot)\n",
    "\n",
    "\n",
    "    # data is a np.array of shape ('b', 's', 'w', 'e') we want to\n",
    "    # pad it with np.zeros of shape ('e',) to get \n",
    "    # ('b', 'SENTENCE_MAX_LENGTH', 'WORD_MAX_LENGTH', 'e')\n",
    "    def numpy_fillna(data):\n",
    "        \"\"\" This is a very useful function that fill the holes in our tensor \"\"\"\n",
    "\n",
    "        # Get lengths of each row of data\n",
    "        lens = np.array([len(i) for i in data])\n",
    "\n",
    "        # Mask of valid places in each row\n",
    "        mask = np.arange(lens.max()) < lens[:, None]\n",
    "        #print(mask)\n",
    "\n",
    "        # Setup output array and put elements from data into masked positions\n",
    "        out = np.zeros(shape=(mask.shape + (max_word_length, alphabet_size)),\n",
    "                       dtype='float32')\n",
    "        #print(out)\n",
    "\n",
    "        out[mask] = np.concatenate(data)\n",
    "        #print(out,'final')\n",
    "        return out\n",
    "\n",
    "    # Padding...\n",
    "    minibatch_x = numpy_fillna(minibatch_x)\n",
    "\n",
    "    return minibatch_x, np.array(minibatch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_x, minibatch_y = make_minibatch(X_all[:15],y_all[:15], MAX_WORD_LENGTH, ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 31, 16, 69)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a generator to feed to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatch(batch_size, X_data, y_data):\n",
    "        \"\"\" Returns Next Batch \"\"\"\n",
    "        \n",
    "        n_samples = len(X_data)\n",
    "        \n",
    "        # Number of batches / number of iterations per epoch\n",
    "        n_batch = int(n_samples // batch_size)\n",
    "        \n",
    "        # Creates a minibatch, loads it to RAM and feed it to the network\n",
    "        # until the buffer is empty\n",
    "        for i in range(n_batch):\n",
    "\n",
    "            inputs, targets = make_minibatch(X_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             y_data[max(batch_size*i-batch_size,0):max(batch_size*i,batch_size)],\\\n",
    "                                             MAX_WORD_LENGTH, ALPHABET_SIZE)\n",
    "            yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "    \"\"\" Straight-forward convvolutional layer \"\"\"\n",
    "    # w is the kernel, b the bias, no strides and VALID padding\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "        b = tf.get_variable('b', [output_dim])\n",
    "\n",
    "    return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdnn(input_, kernels, kernel_features, ALPHABET_SIZE, scope='TDNN'):\n",
    "    ''' Time Delay Neural Network\n",
    "    :input:           input float tensor of shape \n",
    "                      [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "    :kernels:         array of kernel sizes\n",
    "    :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "    '''\n",
    "    assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "\n",
    "    # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'embed_size') we\n",
    "    # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
    "    # use conv2D\n",
    "    # It might not seem obvious why we need to use this small hack at first sight, the reason\n",
    "    # is that sentence_length will change across the different minibatches, but if we kept it\n",
    "    # as is sentence_length would act as the number of channels in the convnet which NEEDS to\n",
    "    # stay the same\n",
    "    input_ = tf.reshape(input_, [-1, self.max_word_length, ALPHABET_SIZE])\n",
    "    input_ = tf.expand_dims(input_, 1)\n",
    "\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope):\n",
    "        for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "            reduced_length = self.max_word_length - kernel_size + 1\n",
    "\n",
    "            # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
    "            conv = conv2d(input_, kernel_feature_size, 1,\n",
    "                          kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "\n",
    "            # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
    "            pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "            layers.append(tf.squeeze(pool, [1, 2]))\n",
    "\n",
    "        if len(kernels) > 1:\n",
    "            output = tf.concat(layers, 1)\n",
    "        else:\n",
    "            output = layers[0]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This goes in ops.py\n",
    "\n",
    "def linear(input_, output_size, scope=None):\n",
    "    \"\"\"\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "    \n",
    "    Args:\n",
    "        args: a tensor or a list of 2D, batch x n, Tensors.\n",
    "        output_size: int, second dimension of W[i].\n",
    "        scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "        \n",
    "    Returns:\n",
    "        A 2D Tensor with shape [batch x output_size] equal to\n",
    "        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size],\n",
    "                                 dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    \"\"\" Character-Level LSTM Implementation \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Get the hyperparameters\n",
    "        self.hparams = self.get_hparams()\n",
    "        \n",
    "        # maximum length of each words\n",
    "        max_word_length = self.hparams['max_word_length']\n",
    "        \n",
    "        # X is of shape ('b', 'sentence_length', 'max_word_length', 'alphabet_size']\n",
    "        # Placeholder for the one-hot encoded sentences\n",
    "        self.X = tf.placeholder('float32', \n",
    "                                shape=[None, None, max_word_length, ALPHABET_SIZE],\n",
    "                                name='X')\n",
    "        \n",
    "        # Placeholder for the one-hot encoded sentiment\n",
    "        self.Y = tf.placeholder('float32', shape=[None, 2], name='Y')\n",
    "        \n",
    "    def get_hparams(self):\n",
    "        ''' Get Hyperparameters '''\n",
    "\n",
    "        return {\n",
    "            'BATCH_SIZE':       64,\n",
    "            'EPOCHS':           500,\n",
    "            'max_word_length':  16,\n",
    "            'learning_rate':    0.0001,\n",
    "            'patience':         10000,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATCH_SIZE': 64,\n",
       " 'EPOCHS': 500,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_word_length': 16,\n",
       " 'patience': 10000}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input_, out_dim, scope=None):\n",
    "    \"\"\" SoftMax Output \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope or 'softmax'):\n",
    "        W = tf.get_variable('W', [input_.get_shape()[1], out_dim])\n",
    "        b = tf.get_variable('b', [out_dim])\n",
    "\n",
    "    return tf.nn.softmax(tf.matmul(input_, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183959,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394653,)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build(self,\n",
    "              training=False,\n",
    "              testing_batch_size=1000,\n",
    "              kernels=[1, 2, 3, 4, 5, 6, 7],\n",
    "              kernel_features=[25, 50, 75, 100, 125, 150, 175],\n",
    "              rnn_size=650,\n",
    "              dropout=0.0,\n",
    "              size=700,\n",
    "              train_samples=1183959,\n",
    "              valid_samples=394653,\n",
    "              hparams= None):\n",
    "        \"\"\"\n",
    "        Build the computational graph\n",
    "        \n",
    "        :param training: \n",
    "            Boolean whether we are training (True) or testing (False)\n",
    "        \n",
    "        :param testing_batch_size: \n",
    "            Batch size to use during testing \n",
    "            \n",
    "        :param kernels:\n",
    "            Kernel width for each convolutional layer\n",
    "         \n",
    "        :param kernel_features: \n",
    "            Number of kernels for each convolutional layer\n",
    "            \n",
    "        :param rnn_size: \n",
    "            Size of the LSTM output\n",
    "        \n",
    "        :param dropout: \n",
    "            Retain probability when using dropout\n",
    "        \n",
    "        :param size: \n",
    "            Size of the Highway embeding\n",
    "        \n",
    "        :param train_samples: \n",
    "            Number of training samples\n",
    "        \n",
    "        :param valid_samples: \n",
    "            Number of validation samples\n",
    "        :hyperparameters\n",
    "            dict of parameters\n",
    "        \"\"\"\n",
    "\n",
    "        max_word_length = hparams['max_word_length']\n",
    "\n",
    "        \n",
    "        # If we are training use the BATCH_SIZE from the hyperparameters\n",
    "        # else use the testing batch size\n",
    "        if training == True:\n",
    "            BATCH_SIZE = self.hparams['BATCH_SIZE']\n",
    "\n",
    "        else:\n",
    "            BATCH_SIZE = testing_batch_size\n",
    "            BATCH_SIZE = BATCH_SIZE\n",
    "        \n",
    "        # Pass the sentences through the CharCNN network\n",
    "        cnn = tdnn(self.X, kernels, kernel_features)\n",
    "\n",
    "        # tdnn() returns a tensor of shape [batch_size * sentence_length x kernel_features]\n",
    "        # highway() returns a tensor of shape [batch_size * sentence_length x size] to use\n",
    "        # tensorflow dynamic_rnn module we need to reshape it to \n",
    "        # [batch_size x sentence_length x size]\n",
    "        cnn = highway(cnn, self.size)\n",
    "        cnn = tf.reshape(cnn, [BATCH_SIZE, -1, self.size])\n",
    "        \n",
    "        # Build the LSTM\n",
    "        with tf.variable_scope('LSTM'):\n",
    "        \n",
    "            # The following is pretty straight-forward, create a cell and add dropout if\n",
    "            # necessary. Note that I did not use dropout to get my results, but using it\n",
    "            # will probably help\n",
    "            def create_rnn_cell():\n",
    "                cell = rnn.BasicLSTMCell(rnn_size, state_is_tuple=True,\n",
    "                                         forget_bias=0.0, reuse=False)\n",
    "\n",
    "                if dropout > 0.0:\n",
    "                    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1. - dropout)\n",
    "\n",
    "                return cell\n",
    "\n",
    "            cell = create_rnn_cell()\n",
    "            \n",
    "            # Initial state of the LSTM cell\n",
    "            initial_rnn_state = cell.zero_state(BATCH_SIZE, dtype='float32')\n",
    "            \n",
    "            # This function returns the outputs at every steps of \n",
    "            # the LSTM (i.e. one output for every word)\n",
    "            outputs, final_rnn_state = tf.nn.dynamic_rnn(cell, cnn,\n",
    "                                                         initial_state=initial_rnn_state,\n",
    "                                                         dtype=tf.float32)\n",
    "\n",
    "            # In this implementation, we only care about the last outputs of the RNN\n",
    "            # i.e. the output at the end of the sentence\n",
    "            outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "            last = outputs[-1]\n",
    "\n",
    "        self.prediction = softmax(last, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 39, 16, 69)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 31, 16, 69)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1 , l = encode_one_hot(X_all[1],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2 , l = encode_one_hot(X_all[2],EMB_ALPHABET, MAX_WORD_LENGTH, ALPHABET_SIZE, ALPHABET_DICT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 16, 69)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 16, 69)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to train two models - LSTM and CNN to sentiment analysis\n",
    "- Using tweets here because the primary source of text from founders will be Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_founder",
   "language": "python",
   "name": "nlp_founder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
